{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec391329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae7570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 26,551 rows, 177 columns\n",
      "Data types: {dtype('float64'): 120, dtype('O'): 55, datetime64[ns, Europe/Budapest]: 3}\n",
      "\n",
      "Datetime columns timezone info:\n",
      "  created: datetime64[ns, Europe/Budapest]\n",
      "  birth_date: datetime64[ns, Europe/Budapest]\n",
      "  ultrasound3_date: datetime64[ns, Europe/Budapest]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created</th>\n",
       "      <th>birth_name</th>\n",
       "      <th>mothers_name</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>birth_place</th>\n",
       "      <th>clinic_name</th>\n",
       "      <th>mep</th>\n",
       "      <th>settlement</th>\n",
       "      <th>mep_region</th>\n",
       "      <th>...</th>\n",
       "      <th>extended_bp8_unknown</th>\n",
       "      <th>extended_bp9_unknown</th>\n",
       "      <th>extended_bp10_unknown</th>\n",
       "      <th>measurements_otoscope_data</th>\n",
       "      <th>measurements_diabetes_data</th>\n",
       "      <th>measurements_cov2_data</th>\n",
       "      <th>icd3_code</th>\n",
       "      <th>pid</th>\n",
       "      <th>taj_present</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-09 12:27:18+02:00</td>\n",
       "      <td>Varga Medárd Rikárdó</td>\n",
       "      <td>Varga Dzsenifer</td>\n",
       "      <td>Varga Medárd Rikárdó</td>\n",
       "      <td>2019-02-04 00:00:00+01:00</td>\n",
       "      <td>Pécs</td>\n",
       "      <td>Drávaiványi rendelő</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drávaiványi</td>\n",
       "      <td>Hirics MEP régió</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A09</td>\n",
       "      <td>8c4a1e6788d9f0dbe446ae0878093a771c1f2d76035d61...</td>\n",
       "      <td>yes</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-09 21:15:48+02:00</td>\n",
       "      <td>Kelemen Mária</td>\n",
       "      <td>Kürti Mária</td>\n",
       "      <td>Ferkó Istvánné</td>\n",
       "      <td>1955-06-11 00:00:00+02:00</td>\n",
       "      <td>Kiskunhalas</td>\n",
       "      <td>Nógrádszakál rendelő</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nógrádszakál</td>\n",
       "      <td>Litke MEP régió</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E74</td>\n",
       "      <td>279159d05b4d9db0bc548ac4db6a934cfbcc6715646876...</td>\n",
       "      <td>yes</td>\n",
       "      <td>69.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 05:07:50+02:00</td>\n",
       "      <td>Kiss Sándor</td>\n",
       "      <td>Fehér Mária</td>\n",
       "      <td>Kiss Sándor</td>\n",
       "      <td>1971-07-09 00:00:00+01:00</td>\n",
       "      <td>Siklós</td>\n",
       "      <td>Hirics rendelő</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hirics</td>\n",
       "      <td>Hirics MEP régió</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I10</td>\n",
       "      <td>7c2a4b0d57e732af7252a5304c1f12718e0d063a199f3b...</td>\n",
       "      <td>yes</td>\n",
       "      <td>51.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-08 11:13:58+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Szilágyi Ilona</td>\n",
       "      <td>Talabos Dávid Csabáné Jéri Edina</td>\n",
       "      <td>1996-05-01 00:00:00+02:00</td>\n",
       "      <td>Nyiregyháza</td>\n",
       "      <td>Nyírkáta rendelő</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nyírkáta</td>\n",
       "      <td>Nyírkáta MEP régió</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z13</td>\n",
       "      <td>8cee43857a01ec5adfb64eba5bf1b57b19bbeb85090d14...</td>\n",
       "      <td>yes</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-22 14:32:28+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bedők Mária</td>\n",
       "      <td>Varjú Lászlóné</td>\n",
       "      <td>1947-07-24 00:00:00+02:00</td>\n",
       "      <td>Zebegény</td>\n",
       "      <td>Központ rendelő</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Központ</td>\n",
       "      <td>Egyéb</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N30</td>\n",
       "      <td>241502f62bc78e65a13679ea5e193c8b9de3b932105847...</td>\n",
       "      <td>yes</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created            birth_name     mothers_name  \\\n",
       "0 2024-07-09 12:27:18+02:00  Varga Medárd Rikárdó  Varga Dzsenifer   \n",
       "1 2024-07-09 21:15:48+02:00         Kelemen Mária      Kürti Mária   \n",
       "2 2023-03-30 05:07:50+02:00           Kiss Sándor      Fehér Mária   \n",
       "3 2023-03-08 11:13:58+01:00                   NaN   Szilágyi Ilona   \n",
       "4 2024-11-22 14:32:28+01:00                   NaN      Bedők Mária   \n",
       "\n",
       "                       patient_name                birth_date  birth_place  \\\n",
       "0              Varga Medárd Rikárdó 2019-02-04 00:00:00+01:00         Pécs   \n",
       "1                    Ferkó Istvánné 1955-06-11 00:00:00+02:00  Kiskunhalas   \n",
       "2                       Kiss Sándor 1971-07-09 00:00:00+01:00       Siklós   \n",
       "3  Talabos Dávid Csabáné Jéri Edina 1996-05-01 00:00:00+02:00  Nyiregyháza   \n",
       "4                    Varjú Lászlóné 1947-07-24 00:00:00+02:00     Zebegény   \n",
       "\n",
       "            clinic_name  mep     settlement          mep_region  ...  \\\n",
       "0   Drávaiványi rendelő  NaN   Drávaiványi     Hirics MEP régió  ...   \n",
       "1  Nógrádszakál rendelő  NaN  Nógrádszakál      Litke MEP régió  ...   \n",
       "2        Hirics rendelő  NaN        Hirics     Hirics MEP régió  ...   \n",
       "3      Nyírkáta rendelő  NaN      Nyírkáta   Nyírkáta MEP régió  ...   \n",
       "4       Központ rendelő  NaN       Központ                Egyéb  ...   \n",
       "\n",
       "  extended_bp8_unknown extended_bp9_unknown extended_bp10_unknown  \\\n",
       "0                  NaN                  NaN                   NaN   \n",
       "1                  NaN                  NaN                   NaN   \n",
       "2                  NaN                  NaN                   NaN   \n",
       "3                  NaN                  NaN                   NaN   \n",
       "4                  NaN                  NaN                   NaN   \n",
       "\n",
       "  measurements_otoscope_data measurements_diabetes_data  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  measurements_cov2_data icd3_code  \\\n",
       "0                    NaN       A09   \n",
       "1                    NaN       E74   \n",
       "2                    NaN       I10   \n",
       "3                    NaN       Z13   \n",
       "4                    NaN       N30   \n",
       "\n",
       "                                                 pid  taj_present   age  \n",
       "0  8c4a1e6788d9f0dbe446ae0878093a771c1f2d76035d61...          yes   5.4  \n",
       "1  279159d05b4d9db0bc548ac4db6a934cfbcc6715646876...          yes  69.1  \n",
       "2  7c2a4b0d57e732af7252a5304c1f12718e0d063a199f3b...          yes  51.7  \n",
       "3  8cee43857a01ec5adfb64eba5bf1b57b19bbeb85090d14...          yes  26.9  \n",
       "4  241502f62bc78e65a13679ea5e193c8b9de3b932105847...          yes  77.3  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../data/patient_records.csv', low_memory=False)\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Hungarian timezone\n",
    "hungarian_tz = pytz.timezone('Europe/Budapest')\n",
    "\n",
    "# Convert date columns with Hungarian timezone\n",
    "# created: full datetime with time - localize to Hungarian timezone\n",
    "# birth_date: date only - parse and localize at midnight\n",
    "# ultrasound3_date: date/datetime - localize to Hungarian timezone\n",
    "date_columns = ['created', 'birth_date', 'ultrasound3_date']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        # Parse as datetime first\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Localize naive datetimes to Hungarian timezone\n",
    "        # Note: If datetime is already timezone-aware, this will raise an error\n",
    "        # so we check if it's naive first\n",
    "        if df[col].dtype == 'datetime64[ns]':\n",
    "            # Check if any non-null values exist\n",
    "            if df[col].notna().any():\n",
    "                # Localize naive datetimes to Hungarian timezone\n",
    "                df[col] = df[col].dt.tz_localize(hungarian_tz, ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "# Parse time columns if they exist\n",
    "time_columns = ['ultrasound4_time1', 'ultrasound5_time2']\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        # Try to parse as time, but keep as string if parsing fails\n",
    "        # These might be time-only values, so we'll handle them separately if needed\n",
    "        pass\n",
    "\n",
    "# Calculate age if birth_date is available\n",
    "# Age calculation works with timezone-aware datetimes\n",
    "if 'birth_date' in df.columns and 'created' in df.columns:\n",
    "    df['age'] = (df['created'] - df['birth_date']).dt.days / 365.25\n",
    "    df['age'] = df['age'].round(1)\n",
    "\n",
    "print(f\"Data types: {df.dtypes.value_counts().to_dict()}\\n\")\n",
    "print(f\"Datetime columns timezone info:\")\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"  {col}: {df[col].dtype}\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b09bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for duplicate rows...\n",
      "Shape before removing duplicates: (26551, 178)\n",
      "Number of duplicate rows: 5\n",
      "Shape after removing duplicates: (26546, 178)\n",
      "Rows removed: 5\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows\n",
    "print(f\"\\nChecking for duplicate rows...\")\n",
    "print(f\"Shape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# Count duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count:,}\")\n",
    "\n",
    "# Drop duplicates (keep first occurrence)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "print(f\"Rows removed: {duplicate_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e54283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating 'created' datetime column...\n",
      "Shape before validation: (26546, 178)\n",
      "Rows with invalid 'created' datetime: 0\n",
      "All rows have valid 'created' datetime ✓\n",
      "Shape after validation: (26546, 178)\n"
     ]
    }
   ],
   "source": [
    "# Ensure created is valid datetime for every row\n",
    "print(f\"\\nValidating 'created' datetime column...\")\n",
    "print(f\"Shape before validation: {df.shape}\")\n",
    "\n",
    "# Check for invalid datetime values (NaT/null)\n",
    "invalid_created = df['created'].isna().sum()\n",
    "print(f\"Rows with invalid 'created' datetime: {invalid_created:,}\")\n",
    "\n",
    "if invalid_created > 0:\n",
    "    # Optionally show sample of invalid rows\n",
    "    if invalid_created <= 10:\n",
    "        print(f\"\\nRows with invalid 'created':\")\n",
    "        print(df[df['created'].isna()][['created', 'pid']].head())\n",
    "    \n",
    "    # Drop rows with invalid created datetime\n",
    "    print(f\"\\nRows removed: {invalid_created:,}\")\n",
    "else:\n",
    "    print(\"All rows have valid 'created' datetime ✓\")\n",
    "\n",
    "print(f\"Shape after validation: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ad0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing bp_systolic_temp to extract numeric values...\n",
      "  Extracted 8,306 numeric systolic BP values from bp_systolic_temp\n",
      "  Updated bp_systolic_temp column with parsed numeric values\n",
      "  bp_systolic now has 8,306 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Parse bp_systolic_temp to extract numeric systolic BP values\n",
    "print(f\"\\nParsing bp_systolic_temp to extract numeric values...\")\n",
    "\n",
    "if 'bp_systolic_temp' in df.columns:\n",
    "    def extract_systolic_bp(value):\n",
    "        \"\"\"\n",
    "        Extract systolic BP from bp_systolic_temp string.\n",
    "        Handles formats like:\n",
    "        - '110 / 76' -> 110 (extract first number)\n",
    "        - '133' -> 133 (single number)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Check if it contains a slash (format: \"SBP / DBP\")\n",
    "        if '/' in value_str:\n",
    "            # Extract first number before the slash\n",
    "            parts = value_str.split('/')\n",
    "            if len(parts) > 0:\n",
    "                first_part = parts[0].strip()\n",
    "                try:\n",
    "                    return float(first_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "        else:\n",
    "            # Single number, try to parse directly\n",
    "            try:\n",
    "                return float(value_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    \n",
    "    # Apply extraction function\n",
    "    parsed_systolic = df['bp_systolic_temp'].apply(extract_systolic_bp)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_systolic.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric systolic BP values from bp_systolic_temp\")\n",
    "    \n",
    "    # Update bp_systolic_temp column with parsed numeric values\n",
    "    df['bp_systolic_temp'] = parsed_systolic\n",
    "    print(f\"  Updated bp_systolic_temp column with parsed numeric values\")\n",
    "    \n",
    "    # Fill missing values in bp_systolic where we have parsed values\n",
    "    if 'bp_systolic' in df.columns:\n",
    "        # Count how many missing bp_systolic values we can fill\n",
    "        missing_bp_systolic = df['bp_systolic'].isna()\n",
    "        can_fill = (missing_bp_systolic & parsed_systolic.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df.loc[missing_bp_systolic & parsed_systolic.notna(), 'bp_systolic'] = \\\n",
    "                parsed_systolic[missing_bp_systolic & parsed_systolic.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing bp_systolic values from bp_systolic_temp\")\n",
    "        \n",
    "        # Optionally, also update existing values if bp_systolic_temp is more complete\n",
    "        # For now, we'll keep existing bp_systolic values and only fill missing ones\n",
    "    else:\n",
    "        # If bp_systolic doesn't exist, create it from parsed values\n",
    "        df['bp_systolic'] = parsed_systolic\n",
    "        print(f\"  Created bp_systolic column from bp_systolic_temp\")\n",
    "    \n",
    "    print(f\"  bp_systolic now has {df['bp_systolic'].notna().sum():,} non-null values\")\n",
    "else:\n",
    "    print(\"  bp_systolic_temp column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5b4aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(df['pid'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d77de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop missing pid\n",
    "\n",
    "df_seg = df[df['pid'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fac7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['pid'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42dd3edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created', 'birth_name', 'mothers_name', 'patient_name', 'birth_date',\n",
      "       'birth_place', 'clinic_name', 'mep', 'settlement', 'mep_region',\n",
      "       ...\n",
      "       'extended_bp8_unknown', 'extended_bp9_unknown', 'extended_bp10_unknown',\n",
      "       'measurements_otoscope_data', 'measurements_diabetes_data',\n",
      "       'measurements_cov2_data', 'icd3_code', 'pid', 'taj_present', 'age'],\n",
      "      dtype='object', length=178)\n"
     ]
    }
   ],
   "source": [
    "print(df_seg.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3a048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop columns not needed\n",
    "columns_to_drop = [\n",
    "\"birth_name\",\n",
    "\"mothers_name\",\n",
    "\"patient_name\",\n",
    "\"birth_date\",\n",
    "\"birth_place\",\n",
    "\"clinic_name\",\n",
    "\"mep\",\n",
    "\"settlement\",\n",
    "\"icd_code_name\",\n",
    "\"prescribed_medication\",\n",
    "\"prescribed_medication_active_ingredient\",\n",
    "\"prescribed_medication_ttt\",\n",
    "\"visit_reason\",\n",
    "\"doctor_stamp\",\n",
    "\"doctor_name\",\n",
    "\"screening_administrative\",\n",
    "\"ultrasound_description\",\n",
    "\"ultrasound1_area_code_id\",\n",
    "\"ultrasound2_machine_code_id\",\n",
    "\"ultrasound4_time1\",\n",
    "\"ultrasound5_time2\",\n",
    "\"referral_institution\",\n",
    "\"referral_specialty\",\n",
    "\"cv_screening1_unknown\",\n",
    "\"cv_screening2_bp\",\n",
    "\"cv_screening3_unknown\",\n",
    "\"cv_screening4_unknown\",\n",
    "\"cv_screening9_unknown\",\n",
    "\"telemedicine1_unknown\",\n",
    "\"telemedicine2_unknown\",\n",
    "\"telemedicine3_unknown\",\n",
    "\"telemedicine4_unknown\",\n",
    "\"telemedicine5_unknown\",\n",
    "\"telemedicine6_unknown\",\n",
    "\"ekg1_description\",\n",
    "\"ekg2_unknown\",\n",
    "\"ekg3_unknown\",\n",
    "\"vision1_unknown\",\n",
    "\"vision2_unknown\",\n",
    "\"vision3_unknown\",\n",
    "\"vision4_unknown\",\n",
    "\"vision5_unknown\",\n",
    "\"vision6_unknown\",\n",
    "\"vision7_unknown\",\n",
    "\"vision8_unknown\",\n",
    "\"vision9_unknown\",\n",
    "\"vision10_unknown\",\n",
    "\"vision11_unknown\",\n",
    "\"vision12_unknown\",\n",
    "\"vision13_unknown\",\n",
    "\"vision14_unknown\",\n",
    "\"vision15_unknown\",\n",
    "\"vision16_unknown\",\n",
    "\"vision17_unknown\",\n",
    "\"vision18_unknown\",\n",
    "\"vision19_unknown\",\n",
    "\"vision20_unknown\",\n",
    "\"vision21_unknown\",\n",
    "\"vision22_unknown\",\n",
    "\"vision23_unknown\",\n",
    "\"vision24_unknown\",\n",
    "\"vision25_unknown\",\n",
    "\"vision26_unknown\",\n",
    "\"vision27_unknown\",\n",
    "\"vision28_unknown\",\n",
    "\"vision29_unknown\",\n",
    "\"vision30_unknown\",\n",
    "\"vision31_unknown\",\n",
    "\"vision32_unknown\",\n",
    "\"vision33_unknown\",\n",
    "\"vision34_unknown\",\n",
    "\"vision35_unknown\",\n",
    "\"vision36_unknown\",\n",
    "\"vision37_unknown\",\n",
    "\"vision38_unknown\",\n",
    "\"vision39_unknown\",\n",
    "\"vision40_unknown\",\n",
    "\"vision41_unknown\",\n",
    "\"vision42_unknown\",\n",
    "\"vision43_unknown\",\n",
    "\"vision44_unknown\",\n",
    "\"vision45_unknown\",\n",
    "\"vision46_unknown\",\n",
    "\"vision47_unknown\",\n",
    "\"vision48_unknown\",\n",
    "\"vision49_unknown\",\n",
    "\"vision50_unknown\",\n",
    "\"vision51_unknown\",\n",
    "\"vision52_unknown\",\n",
    "\"vision53_unknown\",\n",
    "\"vision54_unknown\",\n",
    "\"vision55_unknown\",\n",
    "\"vision56_unknown\",\n",
    "\"vision57_unknown\",\n",
    "\"vision58_unknown\",\n",
    "\"vision59_unknown\",\n",
    "\"vision60_unknown\",\n",
    "\"vision61_unknown\",\n",
    "\"vision62_unknown\",\n",
    "\"vision63_unknown\",\n",
    "\"vision64_unknown\",\n",
    "\"vision65_unknown\",\n",
    "\"vision66_unknown\",\n",
    "\"vision67_unknown\",\n",
    "\"vision68_unknown\",\n",
    "\"vision69_unknown\",\n",
    "\"vision70_unknown\",\n",
    "\"vision71_unknown\",\n",
    "\"vision72_unknown\",\n",
    "\"vision73_unknown\",\n",
    "\"vision74_unknown\",\n",
    "\"vision75_unknown\",\n",
    "\"vision76_unknown\",\n",
    "\"vision77_unknown\",\n",
    "\"vision78_unknown\",\n",
    "\"vision79_unknown\",\n",
    "\"vision80_unknown\",\n",
    "\"vision81_unknown\",\n",
    "\"vision82_unknown\",\n",
    "\"vision83_unknown\",\n",
    "\"vision84_unknown\",\n",
    "\"vision85_unknown\",\n",
    "\"vision86_unknown\",\n",
    "\"vision87_unknown\",\n",
    "\"vision88_unknown\",\n",
    "\"vision89_unknown\",\n",
    "\"vision90_unknown\",\n",
    "\"vision91_unknown\",\n",
    "\"vision93_unknown\",\n",
    "\"vision94_unknown\",\n",
    "\"vision95_unknown\",\n",
    "\"vision96_unknown\",\n",
    "\"children_dentistry1_unknown\",\n",
    "\"children_dentistry2_unknown\",\n",
    "\"osas1_unknown\",\n",
    "\"osas2_unknown\",\n",
    "\"measurements_respiratory_function_data\",\n",
    "\"measurements_arteriography_data\",\n",
    "\"extended_bp1_unknown\",\n",
    "\"extended_bp2_unknown\",\n",
    "\"extended_bp3_unknown\",\n",
    "\"extended_bp4_unknown\",\n",
    "\"extended_bp5_unknown\",\n",
    "\"extended_bp6_unknown\",\n",
    "\"extended_bp7_unknown\",\n",
    "\"extended_bp8_unknown\",\n",
    "\"extended_bp9_unknown\",\n",
    "\"extended_bp10_unknown\",\n",
    "\"measurements_otoscope_data\",\n",
    "\"measurements_cov2_data\",\n",
    "\"taj_present\",\n",
    "\"measurements_diabetes_data\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20412cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop columns not needed\n",
    "\n",
    "df_seg.drop(columns=columns_to_drop, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e23e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26508, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f7d97a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created', 'mep_region', 'patient_gender', 'prescribed_medication_atc',\n",
       "       'specialty_name', 'measurements_ultrasound_category',\n",
       "       'ultrasound3_date', 'bp_systolic_temp', 'bp_systolic', 'bp_diastolic',\n",
       "       'bp_systolic2', 'bp_diastolic2', 'pulse', 'cv_screening5_height',\n",
       "       'cv_screening6_weight', 'cv_screening7_bmi',\n",
       "       'cv_screening8_waist_circumference', 'physical1_height',\n",
       "       'physical2_weight', 'physical3_bmi', 'physical4_waist_circumference',\n",
       "       'pulse_oximetry1_saturation', 'pulse_oximetry2_pulse', 'icd3_code',\n",
       "       'pid', 'age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd385ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26508 entries, 0 to 26550\n",
      "Data columns (total 26 columns):\n",
      " #   Column                             Non-Null Count  Dtype                          \n",
      "---  ------                             --------------  -----                          \n",
      " 0   created                            26508 non-null  datetime64[ns, Europe/Budapest]\n",
      " 1   mep_region                         26508 non-null  object                         \n",
      " 2   patient_gender                     26508 non-null  object                         \n",
      " 3   prescribed_medication_atc          6681 non-null   object                         \n",
      " 4   specialty_name                     26508 non-null  object                         \n",
      " 5   measurements_ultrasound_category   4076 non-null   object                         \n",
      " 6   ultrasound3_date                   3794 non-null   datetime64[ns, Europe/Budapest]\n",
      " 7   bp_systolic_temp                   8294 non-null   float64                        \n",
      " 8   bp_systolic                        8294 non-null   float64                        \n",
      " 9   bp_diastolic                       8229 non-null   float64                        \n",
      " 10  bp_systolic2                       4990 non-null   float64                        \n",
      " 11  bp_diastolic2                      4885 non-null   float64                        \n",
      " 12  pulse                              8499 non-null   float64                        \n",
      " 13  cv_screening5_height               3241 non-null   object                         \n",
      " 14  cv_screening6_weight               3232 non-null   object                         \n",
      " 15  cv_screening7_bmi                  3242 non-null   float64                        \n",
      " 16  cv_screening8_waist_circumference  3028 non-null   object                         \n",
      " 17  physical1_height                   6316 non-null   object                         \n",
      " 18  physical2_weight                   5798 non-null   object                         \n",
      " 19  physical3_bmi                      6311 non-null   float64                        \n",
      " 20  physical4_waist_circumference      4472 non-null   object                         \n",
      " 21  pulse_oximetry1_saturation         2690 non-null   object                         \n",
      " 22  pulse_oximetry2_pulse              1554 non-null   object                         \n",
      " 23  icd3_code                          26494 non-null  object                         \n",
      " 24  pid                                26508 non-null  object                         \n",
      " 25  age                                26506 non-null  float64                        \n",
      "dtypes: datetime64[ns, Europe/Budapest](2), float64(9), object(15)\n",
      "memory usage: 5.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bp_systolic_temp</th>\n",
       "      <th>bp_systolic</th>\n",
       "      <th>bp_diastolic</th>\n",
       "      <th>bp_systolic2</th>\n",
       "      <th>bp_diastolic2</th>\n",
       "      <th>pulse</th>\n",
       "      <th>cv_screening7_bmi</th>\n",
       "      <th>physical3_bmi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8294.000000</td>\n",
       "      <td>8294.000000</td>\n",
       "      <td>8229.000000</td>\n",
       "      <td>4990.000000</td>\n",
       "      <td>4885.000000</td>\n",
       "      <td>8499.000000</td>\n",
       "      <td>3.242000e+03</td>\n",
       "      <td>6311.000000</td>\n",
       "      <td>26506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>134.598987</td>\n",
       "      <td>134.598987</td>\n",
       "      <td>84.976303</td>\n",
       "      <td>133.658918</td>\n",
       "      <td>84.979939</td>\n",
       "      <td>79.660313</td>\n",
       "      <td>3.472270e+02</td>\n",
       "      <td>30.815490</td>\n",
       "      <td>48.294763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.010447</td>\n",
       "      <td>20.010447</td>\n",
       "      <td>23.036153</td>\n",
       "      <td>21.728520</td>\n",
       "      <td>25.062968</td>\n",
       "      <td>12.773219</td>\n",
       "      <td>1.791433e+04</td>\n",
       "      <td>126.198131</td>\n",
       "      <td>20.369219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.146000e+01</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>2.461000e+01</td>\n",
       "      <td>24.135000</td>\n",
       "      <td>35.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.884000e+01</td>\n",
       "      <td>28.480000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>3.367000e+01</td>\n",
       "      <td>33.310000</td>\n",
       "      <td>64.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>230.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>1.020000e+06</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>96.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bp_systolic_temp  bp_systolic  bp_diastolic  bp_systolic2  \\\n",
       "count       8294.000000  8294.000000   8229.000000   4990.000000   \n",
       "mean         134.598987   134.598987     84.976303    133.658918   \n",
       "std           20.010447    20.010447     23.036153     21.728520   \n",
       "min           77.000000    77.000000      4.000000      1.000000   \n",
       "25%          121.000000   121.000000     77.000000    119.000000   \n",
       "50%          133.000000   133.000000     84.000000    132.000000   \n",
       "75%          147.000000   147.000000     92.000000    147.000000   \n",
       "max          230.000000   230.000000    830.000000    222.000000   \n",
       "\n",
       "       bp_diastolic2        pulse  cv_screening7_bmi  physical3_bmi  \\\n",
       "count    4885.000000  8499.000000       3.242000e+03    6311.000000   \n",
       "mean       84.979939    79.660313       3.472270e+02      30.815490   \n",
       "std        25.062968    12.773219       1.791433e+04     126.198131   \n",
       "min         2.000000     7.000000       1.146000e+01       1.460000   \n",
       "25%        76.000000    71.000000       2.461000e+01      24.135000   \n",
       "50%        84.000000    79.000000       2.884000e+01      28.480000   \n",
       "75%        92.000000    88.000000       3.367000e+01      33.310000   \n",
       "max       780.000000   165.000000       1.020000e+06   10000.000000   \n",
       "\n",
       "                age  \n",
       "count  26506.000000  \n",
       "mean      48.294763  \n",
       "std       20.369219  \n",
       "min        0.000000  \n",
       "25%       35.900000  \n",
       "50%       51.000000  \n",
       "75%       64.600000  \n",
       "max       96.300000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.info()\n",
    "df_seg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4514b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mep_region\n",
       "Nyírkáta MEP régió     7053\n",
       "Litke MEP régió        5326\n",
       "Zalakomár MEP régió    4755\n",
       "Szalonna MEP régió     4264\n",
       "Hirics MEP régió       2837\n",
       "Egyéb                  1717\n",
       "Heves praxis            285\n",
       "Máltai Iskolák          146\n",
       "Ápolási intézmények     125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['mep_region'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0885541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_gender\n",
       "nő       18440\n",
       "férfi     8068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['patient_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f257b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'J01XX01', 'S01AE05, S01AA12', ..., 'C08GA02',\n",
       "       'C02AC06, C10AA07, C09DB04, C07AB12, C03BA11, C02CA04',\n",
       "       'C10AA07, N02AJ14'], shape=(2160,), dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['prescribed_medication_atc'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8d4f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing cv_screening5_height to extract numeric values...\n",
      "  Extracted 3,241 numeric height values from cv_screening5_height\n",
      "  Updated cv_screening5_height column with parsed numeric values (dtype: float64)\n",
      "  Filled 28 missing physical1_height values from cv_screening5_height\n",
      "  cv_screening5_height now has 3,241 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse cv_screening5_height to extract numeric height values\n",
    "print(f\"\\nParsing cv_screening5_height to extract numeric values...\")\n",
    "\n",
    "if 'cv_screening5_height' in df_seg.columns:\n",
    "    def extract_height(value):\n",
    "        \"\"\"\n",
    "        Extract numeric height from cv_screening5_height string.\n",
    "        Handles formats like:\n",
    "        - '153' -> 153 (simple number)\n",
    "        - '158,8' -> 158.8 (comma decimal separator - Hungarian format)\n",
    "        - '165.0' -> 165.0 (dot decimal separator)\n",
    "        - '108 76' -> 108 (multiple numbers, take first)\n",
    "        - '\\xa0173 ' -> 173 (whitespace/special characters, clean and extract)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 158,8 -> 158.8)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            return float(value_str)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_height = df_seg['cv_screening5_height'].apply(extract_height)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_height.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric height values from cv_screening5_height\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['cv_screening5_height'] = parsed_height.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated cv_screening5_height column with parsed numeric values (dtype: {df_seg['cv_screening5_height'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in physical1_height if it exists and we have parsed values\n",
    "    if 'physical1_height' in df_seg.columns:\n",
    "        missing_physical_height = df_seg['physical1_height'].isna()\n",
    "        can_fill = (missing_physical_height & parsed_height.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_physical_height & parsed_height.notna(), 'physical1_height'] = \\\n",
    "                parsed_height[missing_physical_height & parsed_height.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing physical1_height values from cv_screening5_height\")\n",
    "    \n",
    "    print(f\"  cv_screening5_height now has {df_seg['cv_screening5_height'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['cv_screening5_height'].dtype}\")\n",
    "else:\n",
    "    print(\"  cv_screening5_height column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6f7c1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '98.0', '109.0', '100.0', '107.0', '121.0', '93.0', '70.0',\n",
       "       '94.0', '73.0', '106.0', '97.0', '96.0', '112.0', '90.0', '95.0',\n",
       "       '130.0', '83.0', '129.0', '82.0', '91.0', '105.0', '110.0', '92.0',\n",
       "       '76.0', '67.0', '99.0', '115.0', '89.0', '111.0', '84.0', '77.0',\n",
       "       '88.0', '101.0', '119.0', '104.0', '85.0', '116.0', '87.0', '79.0',\n",
       "       '80.0', '108.0', '132.0', '103.0', '72.0', '102.0', '124.0',\n",
       "       '118.0', '62.0', '136.0', '74.0', '135.0', '113.0', '68.0', '81.0',\n",
       "       '117.0', '122.0', '71.0', '123.0', '86.0', '69.0', '120.0',\n",
       "       '114.0', '134.0', '60.0', '78.0', '140.0', '128.0', '127.0',\n",
       "       '126.0', '57.0', '75.0', '147.0', '148.0', '139.0', '133.0',\n",
       "       '131.0', '66.0', '141.0', '63.0', '96', '105', '86', '115', '123',\n",
       "       '136', '121', '103', '94', '84', '130', '91', '81', '98', '80',\n",
       "       '111', '97', '87', '85', '109', '72', '88', '92', '104', '112',\n",
       "       '116', '108', '110', '118', '89', '113', '107', '100', '95', '99',\n",
       "       '67', '93', '117', '102', '77', '122', '106', '101', '119', '78',\n",
       "       '132', '126', '120', '90', '70', '114', 'gravidaként nem mértünk',\n",
       "       '124', '139', '73', '148', '133', '79', '60', '125', '82', '46',\n",
       "       '129', '83', '54', '75', '66', '76', '127', '176', '71', '103\\xa0',\n",
       "       '65', '134', '82,5', '106 ', '131', '69', '140', '137', '74',\n",
       "       '154', '138', '128', '98,5', '152', '146', '156', '143', '135',\n",
       "       '142', '24', '68', '52', '62', '141', '63', '106,5', '144.0',\n",
       "       '64.0', '137.0', '50.0', '125.0', '152.0', '65.0', '138.0', '150',\n",
       "       ' 72', '64', '160', '113\\xa0', '144', '143.0'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['cv_screening8_waist_circumference'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b4083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing cv_screening6_weight to extract numeric values...\n",
      "  Extracted 3,230 numeric weight values from cv_screening6_weight\n",
      "  Updated cv_screening6_weight column with parsed numeric values (dtype: float64)\n",
      "  Filled 752 missing physical2_weight values from cv_screening6_weight\n",
      "  cv_screening6_weight now has 3,230 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse cv_screening6_weight to extract numeric weight values\n",
    "print(f\"\\nParsing cv_screening6_weight to extract numeric values...\")\n",
    "\n",
    "if 'cv_screening6_weight' in df_seg.columns:\n",
    "    def extract_weight(value):\n",
    "        \"\"\"\n",
    "        Extract numeric weight from cv_screening6_weight string.\n",
    "        Handles formats like:\n",
    "        - '68' -> 68 (simple number)\n",
    "        - '76,5' -> 76.5 (comma decimal separator - Hungarian format)\n",
    "        - '80.3' -> 80.3 (dot decimal separator)\n",
    "        - '62,5 ' -> 62.5 (whitespace/special characters, clean and extract)\n",
    "        - '\\xa0104,3' -> 104.3 (non-breaking space, clean and extract)\n",
    "        - '5300' or '5700' -> NaN (likely data entry errors, too high for kg)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 76,5 -> 76.5)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            weight = float(value_str)\n",
    "            # Filter out obvious data entry errors (weights > 1000 kg are likely in grams, not kg)\n",
    "            if weight > 1000:\n",
    "                return np.nan\n",
    "            return weight\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_weight = df_seg['cv_screening6_weight'].apply(extract_weight)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_weight.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric weight values from cv_screening6_weight\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['cv_screening6_weight'] = parsed_weight.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated cv_screening6_weight column with parsed numeric values (dtype: {df_seg['cv_screening6_weight'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in physical2_weight if it exists and we have parsed values\n",
    "    if 'physical2_weight' in df_seg.columns:\n",
    "        missing_physical_weight = df_seg['physical2_weight'].isna()\n",
    "        can_fill = (missing_physical_weight & parsed_weight.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_physical_weight & parsed_weight.notna(), 'physical2_weight'] = \\\n",
    "                parsed_weight[missing_physical_weight & parsed_weight.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing physical2_weight values from cv_screening6_weight\")\n",
    "    \n",
    "    print(f\"  cv_screening6_weight now has {df_seg['cv_screening6_weight'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['cv_screening6_weight'].dtype}\")\n",
    "else:\n",
    "    print(\"  cv_screening6_weight column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48c7603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '98.0', '109.0', '100.0', '107.0', '121.0', '93.0', '70.0',\n",
       "       '94.0', '73.0', '106.0', '97.0', '96.0', '112.0', '90.0', '95.0',\n",
       "       '130.0', '83.0', '129.0', '82.0', '91.0', '105.0', '110.0', '92.0',\n",
       "       '76.0', '67.0', '99.0', '115.0', '89.0', '111.0', '84.0', '77.0',\n",
       "       '88.0', '101.0', '119.0', '104.0', '85.0', '116.0', '87.0', '79.0',\n",
       "       '80.0', '108.0', '132.0', '103.0', '72.0', '102.0', '124.0',\n",
       "       '118.0', '62.0', '136.0', '74.0', '135.0', '113.0', '68.0', '81.0',\n",
       "       '117.0', '122.0', '71.0', '123.0', '86.0', '69.0', '120.0',\n",
       "       '114.0', '134.0', '60.0', '78.0', '140.0', '128.0', '127.0',\n",
       "       '126.0', '57.0', '75.0', '147.0', '148.0', '139.0', '133.0',\n",
       "       '131.0', '66.0', '141.0', '63.0', '96', '105', '86', '115', '123',\n",
       "       '136', '121', '103', '94', '84', '130', '91', '81', '98', '80',\n",
       "       '111', '97', '87', '85', '109', '72', '88', '92', '104', '112',\n",
       "       '116', '108', '110', '118', '89', '113', '107', '100', '95', '99',\n",
       "       '67', '93', '117', '102', '77', '122', '106', '101', '119', '78',\n",
       "       '132', '126', '120', '90', '70', '114', 'gravidaként nem mértünk',\n",
       "       '124', '139', '73', '148', '133', '79', '60', '125', '82', '46',\n",
       "       '129', '83', '54', '75', '66', '76', '127', '176', '71', '103\\xa0',\n",
       "       '65', '134', '82,5', '106 ', '131', '69', '140', '137', '74',\n",
       "       '154', '138', '128', '98,5', '152', '146', '156', '143', '135',\n",
       "       '142', '24', '68', '52', '62', '141', '63', '106,5', '144.0',\n",
       "       '64.0', '137.0', '50.0', '125.0', '152.0', '65.0', '138.0', '150',\n",
       "       ' 72', '64', '160', '113\\xa0', '144', '143.0'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['cv_screening8_waist_circumference'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1691c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing cv_screening8_waist_circumference to extract numeric values...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 3,027 numeric waist circumference values from cv_screening8_waist_circumference\n",
      "  Updated cv_screening8_waist_circumference column with parsed numeric values (dtype: float64)\n",
      "  Filled 1,067 missing physical4_waist_circumference values from cv_screening8_waist_circumference\n",
      "  cv_screening8_waist_circumference now has 3,027 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse cv_screening8_waist_circumference to extract numeric waist circumference values\n",
    "print(f\"\\nParsing cv_screening8_waist_circumference to extract numeric values...\")\n",
    "\n",
    "if 'cv_screening8_waist_circumference' in df_seg.columns:\n",
    "    def extract_waist_circumference(value):\n",
    "        \"\"\"\n",
    "        Extract numeric waist circumference from cv_screening8_waist_circumference string.\n",
    "        Handles formats like:\n",
    "        - '96' -> 96 (simple number)\n",
    "        - '98.0' -> 98.0 (dot decimal separator)\n",
    "        - '82,5' -> 82.5 (comma decimal separator - Hungarian format)\n",
    "        - '106 ' -> 106 (whitespace, clean and extract)\n",
    "        - '103\\xa0' -> 103 (non-breaking space, clean and extract)\n",
    "        - 'gravidaként nem mértünk' -> NaN (text values indicating not measured)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Check if it's a text value indicating not measured (e.g., \"gravidaként nem mértünk\")\n",
    "        # Convert to lowercase for comparison\n",
    "        value_lower = value_str.lower()\n",
    "        if any(keyword in value_lower for keyword in ['nem', 'not', 'mért', 'measured', 'n/a', 'na']):\n",
    "            return np.nan\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 82,5 -> 82.5)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            return float(value_str)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_waist = df_seg['cv_screening8_waist_circumference'].apply(extract_waist_circumference)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_waist.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric waist circumference values from cv_screening8_waist_circumference\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['cv_screening8_waist_circumference'] = parsed_waist.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated cv_screening8_waist_circumference column with parsed numeric values (dtype: {df_seg['cv_screening8_waist_circumference'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in physical4_waist_circumference if it exists and we have parsed values\n",
    "    if 'physical4_waist_circumference' in df_seg.columns:\n",
    "        missing_physical_waist = df_seg['physical4_waist_circumference'].isna()\n",
    "        can_fill = (missing_physical_waist & parsed_waist.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_physical_waist & parsed_waist.notna(), 'physical4_waist_circumference'] = \\\n",
    "                parsed_waist[missing_physical_waist & parsed_waist.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing physical4_waist_circumference values from cv_screening8_waist_circumference\")\n",
    "    \n",
    "    print(f\"  cv_screening8_waist_circumference now has {df_seg['cv_screening8_waist_circumference'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['cv_screening8_waist_circumference'].dtype}\")\n",
    "else:\n",
    "    print(\"  cv_screening8_waist_circumference column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8608cc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26508 entries, 0 to 26550\n",
      "Data columns (total 26 columns):\n",
      " #   Column                             Non-Null Count  Dtype                          \n",
      "---  ------                             --------------  -----                          \n",
      " 0   created                            26508 non-null  datetime64[ns, Europe/Budapest]\n",
      " 1   mep_region                         26508 non-null  object                         \n",
      " 2   patient_gender                     26508 non-null  object                         \n",
      " 3   prescribed_medication_atc          6681 non-null   object                         \n",
      " 4   specialty_name                     26508 non-null  object                         \n",
      " 5   measurements_ultrasound_category   4076 non-null   object                         \n",
      " 6   ultrasound3_date                   3794 non-null   datetime64[ns, Europe/Budapest]\n",
      " 7   bp_systolic_temp                   8294 non-null   float64                        \n",
      " 8   bp_systolic                        8294 non-null   float64                        \n",
      " 9   bp_diastolic                       8229 non-null   float64                        \n",
      " 10  bp_systolic2                       4990 non-null   float64                        \n",
      " 11  bp_diastolic2                      4885 non-null   float64                        \n",
      " 12  pulse                              8499 non-null   float64                        \n",
      " 13  cv_screening5_height               3241 non-null   float64                        \n",
      " 14  cv_screening6_weight               3230 non-null   float64                        \n",
      " 15  cv_screening7_bmi                  3242 non-null   float64                        \n",
      " 16  cv_screening8_waist_circumference  3027 non-null   float64                        \n",
      " 17  physical1_height                   6344 non-null   object                         \n",
      " 18  physical2_weight                   6550 non-null   object                         \n",
      " 19  physical3_bmi                      6311 non-null   float64                        \n",
      " 20  physical4_waist_circumference      5539 non-null   object                         \n",
      " 21  pulse_oximetry1_saturation         2690 non-null   object                         \n",
      " 22  pulse_oximetry2_pulse              1554 non-null   object                         \n",
      " 23  icd3_code                          26494 non-null  object                         \n",
      " 24  pid                                26508 non-null  object                         \n",
      " 25  age                                26506 non-null  float64                        \n",
      "dtypes: datetime64[ns, Europe/Budapest](2), float64(12), object(12)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_seg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "741c35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique values: 131\n",
      "\n",
      "All unique values:\n",
      "[nan, '66', '100', '64', '75', '80', '71', '74', '79', '90', '63', '89', '70', '56', '87', '82', '129', '65', '97', '76', '66/min', '110', '130', '85', '54', '72', '101', '58', '62', '120', '108', '84', '92', '60', '95', '81', '98', '78', '83', '67', '77', '61', '91', '86', '88', '104', '68', '51', '50', '105', '57', '106', '69', '59', '73', '93', '36.8', '96', '138', '52', '116', '99', '79.0', '97.0', '78.0', '96.0', '55.0', '88.0', '102.0', '87.0', '76.0', '112.0', '84.0', '89.0', '95.0', '9064.0', '71.0', '64.0', '83.0', '98.0', '80.0', '68.0', '107.0', '86.0', '53.0', '70.0', '67.0', '62.0', '77.0', '118.0', '85.0', '92.0', '82.0', '94.0', '91.0', '73.0', '50.0', '72.0', '58.0', '66.0', '115.0', '100.0', '101.0', '81.0', '65.0', '104.0', '59.0', '60.0', '90.0', '103.0', '63.0', '74.0', '52.0', '108.0', '54.0', '99.0', '75.0', '61.0', '169.0', '69.0', '110.0', '57.0', '47.0', '105.0', '93.0', '51.0', '116.0', '120.0', '106.0', '56.0', '128.0']\n"
     ]
    }
   ],
   "source": [
    "##show all unique values pf physical2_weight\n",
    "# Display all unique values without truncation\n",
    "unique_values = df_seg['pulse_oximetry2_pulse'].unique()\n",
    "print(f\"Total unique values: {len(unique_values)}\")\n",
    "print(\"\\nAll unique values:\")\n",
    "# Convert to list and print to avoid numpy array truncation\n",
    "print(list(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e91150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing physical1_height to extract numeric values...\n",
      "  Extracted 6,343 numeric height values from physical1_height\n",
      "  Updated physical1_height column with parsed numeric values (dtype: float64)\n",
      "  Filled 3,102 missing cv_screening5_height values from physical1_height\n",
      "  physical1_height now has 6,343 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse physical1_height to extract numeric height values\n",
    "print(f\"\\nParsing physical1_height to extract numeric values...\")\n",
    "\n",
    "if 'physical1_height' in df_seg.columns:\n",
    "    def extract_physical_height(value):\n",
    "        \"\"\"\n",
    "        Extract numeric height from physical1_height string.\n",
    "        Handles formats like:\n",
    "        - '153' -> 153 (simple number)\n",
    "        - 155.0 -> 155.0 (already numeric)\n",
    "        - '162,5' -> 162.5 (comma decimal separator - Hungarian format)\n",
    "        - '162.5' -> 162.5 (dot decimal separator)\n",
    "        - '153 ' -> 153 (whitespace, clean and extract)\n",
    "        - '\\xa0173 ' -> 173 (non-breaking space, clean and extract)\n",
    "        - '108 76' -> 108 (multiple numbers, take first)\n",
    "        - '138 cm' -> 138 (remove unit)\n",
    "        - '160.-' -> 160 (remove trailing dash)\n",
    "        - 'nem mérhető' -> NaN (text indicating not measurable)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # If already numeric, return as is\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Check if it's a text value indicating not measurable (e.g., \"nem mérhető\")\n",
    "        value_lower = value_str.lower()\n",
    "        if any(keyword in value_lower for keyword in ['nem', 'not', 'mérhető', 'measurable', 'n/a', 'na']):\n",
    "            return np.nan\n",
    "        \n",
    "        # Remove units (cm, etc.)\n",
    "        value_str = re.sub(r'\\s*(cm|kg|g)\\s*$', '', value_str, flags=re.IGNORECASE)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Remove trailing dashes or dots (e.g., '160.-' -> '160')\n",
    "        value_str = re.sub(r'[.-]+$', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 162,5 -> 162.5)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            return float(value_str)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_height = df_seg['physical1_height'].apply(extract_physical_height)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_height.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric height values from physical1_height\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['physical1_height'] = parsed_height.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated physical1_height column with parsed numeric values (dtype: {df_seg['physical1_height'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in cv_screening5_height if it exists and we have parsed values\n",
    "    if 'cv_screening5_height' in df_seg.columns:\n",
    "        missing_cv_height = df_seg['cv_screening5_height'].isna()\n",
    "        can_fill = (missing_cv_height & parsed_height.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_cv_height & parsed_height.notna(), 'cv_screening5_height'] = \\\n",
    "                parsed_height[missing_cv_height & parsed_height.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing cv_screening5_height values from physical1_height\")\n",
    "    \n",
    "    print(f\"  physical1_height now has {df_seg['physical1_height'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['physical1_height'].dtype}\")\n",
    "else:\n",
    "    print(\"  physical1_height column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ef142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  nan, 153. , 167. , 168. , 164. , 161. , 160. , 152. , 155. ,\n",
       "       156. , 183. , 171. , 150. , 165. , 134. , 157. , 176. , 175. ,\n",
       "       162. , 177. , 159. , 163. , 154. , 169. , 170. ,  91. , 110. ,\n",
       "       146. , 112. , 131. , 102. , 180. , 172. , 158. , 137. , 124. ,\n",
       "       173. , 119. ,  68. , 149. , 120. , 101. , 143. , 130. , 116. ,\n",
       "       118. , 114. , 185. , 144. , 108. , 178. , 151. , 145. , 186. ,\n",
       "       174. , 166. , 122. ,  64. , 182. ,  67. , 135. ,  78. , 142. ,\n",
       "       140. , 181. , 147. , 148. , 184. , 179. , 187. , 141. , 192. ,\n",
       "       109. , 100. , 188. , 196. ,  99. , 198. , 197. , 162.5, 189. ,\n",
       "       138. ,   1. , 195. , 185.5,  86. , 191. ,  80. , 160.5, 177.5,\n",
       "       113. ,  72. ,  63. ,  83. ,  88.7, 105. , 117. ,  98. , 135.5,\n",
       "        93. , 133. , 129. , 190. , 158.8, 194. ,  53. , 127. , 165.5,\n",
       "       128. , 155.5, 139. , 167.5, 174.5, 136. ,  62. ,  71. , 103. ,\n",
       "       183.5, 173.5,  74. ,  84. ,  96. , 104. , 178.5, 107. ,  42.5,\n",
       "       193. , 132. ,  95. , 125. ,  92. ,  87. ,  57. , 161.5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg['physical1_height'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e21876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing physical2_weight to extract numeric values...\n",
      "  Extracted 6,539 numeric weight values from physical2_weight\n",
      "  Updated physical2_weight column with parsed numeric values (dtype: float64)\n",
      "  Filled 3,309 missing cv_screening6_weight values from physical2_weight\n",
      "  physical2_weight now has 6,539 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse physical2_weight to extract numeric weight values\n",
    "print(f\"\\nParsing physical2_weight to extract numeric values...\")\n",
    "\n",
    "if 'physical2_weight' in df_seg.columns:\n",
    "    def extract_physical_weight(value):\n",
    "        \"\"\"\n",
    "        Extract numeric weight from physical2_weight string.\n",
    "        Handles formats like:\n",
    "        - '68' -> 68 (simple number)\n",
    "        - 65.0 -> 65.0 (already numeric)\n",
    "        - '76,5' -> 76.5 (comma decimal separator - Hungarian format)\n",
    "        - '76.5' -> 76.5 (dot decimal separator)\n",
    "        - '62,5 ' -> 62.5 (whitespace, clean and extract)\n",
    "        - '44.7\\xa0' -> 44.7 (non-breaking space, clean and extract)\n",
    "        - '\\xa0104,3' -> 104.3 (non-breaking space at start)\n",
    "        - '31,8 kg' -> 31.8 (remove unit)\n",
    "        - '50,5 kg' -> 50.5 (remove unit)\n",
    "        - '54,7kg' -> 54.7 (remove unit, no space)\n",
    "        - '5300', '6190', '5700' -> NaN (likely data entry errors, too high for kg)\n",
    "        - 'nem mérhető' -> NaN (text indicating not measurable)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # If already numeric, return as is\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Check if it's a text value indicating not measurable (e.g., \"nem mérhető\")\n",
    "        value_lower = value_str.lower()\n",
    "        if any(keyword in value_lower for keyword in ['nem', 'not', 'mérhető', 'measurable', 'n/a', 'na']):\n",
    "            return np.nan\n",
    "        \n",
    "        # Remove units (kg, g, etc.) - handle both with and without space\n",
    "        value_str = re.sub(r'\\s*(kg|g)\\s*$', '', value_str, flags=re.IGNORECASE)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 76,5 -> 76.5)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            weight = float(value_str)\n",
    "            # Filter out obvious data entry errors (weights > 1000 kg are likely in grams, not kg)\n",
    "            if weight > 1000:\n",
    "                return np.nan\n",
    "            return weight\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_weight = df_seg['physical2_weight'].apply(extract_physical_weight)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_weight.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric weight values from physical2_weight\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['physical2_weight'] = parsed_weight.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated physical2_weight column with parsed numeric values (dtype: {df_seg['physical2_weight'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in cv_screening6_weight if it exists and we have parsed values\n",
    "    if 'cv_screening6_weight' in df_seg.columns:\n",
    "        missing_cv_weight = df_seg['cv_screening6_weight'].isna()\n",
    "        can_fill = (missing_cv_weight & parsed_weight.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_cv_weight & parsed_weight.notna(), 'cv_screening6_weight'] = \\\n",
    "                parsed_weight[missing_cv_weight & parsed_weight.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing cv_screening6_weight values from physical2_weight\")\n",
    "    \n",
    "    print(f\"  physical2_weight now has {df_seg['physical2_weight'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['physical2_weight'].dtype}\")\n",
    "else:\n",
    "    print(\"  physical2_weight column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd8a8e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing physical4_waist_circumference to extract numeric values...\n",
      "  Extracted 5,538 numeric waist circumference values from physical4_waist_circumference\n",
      "  Updated physical4_waist_circumference column with parsed numeric values (dtype: float64)\n",
      "  Filled 2,511 missing cv_screening8_waist_circumference values from physical4_waist_circumference\n",
      "  physical4_waist_circumference now has 5,538 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse physical4_waist_circumference to extract numeric waist circumference values\n",
    "print(f\"\\nParsing physical4_waist_circumference to extract numeric values...\")\n",
    "\n",
    "if 'physical4_waist_circumference' in df_seg.columns:\n",
    "    def extract_physical_waist(value):\n",
    "        \"\"\"\n",
    "        Extract numeric waist circumference from physical4_waist_circumference string.\n",
    "        Handles formats like:\n",
    "        - '98' -> 98 (simple number)\n",
    "        - 98.0 -> 98.0 (already numeric)\n",
    "        - '82,5' -> 82.5 (comma decimal separator - Hungarian format)\n",
    "        - '106,5' -> 106.5 (comma decimal)\n",
    "        - '106 ' -> 106 (whitespace, clean and extract)\n",
    "        - ' 112' -> 112 (leading whitespace)\n",
    "        - '103\\xa0' -> 103 (non-breaking space, clean and extract)\n",
    "        - '113\\xa0' -> 113 (non-breaking space)\n",
    "        - 'gravidaként nem mértünk' -> NaN (text indicating not measured)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # If already numeric, return as is\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove any non-breaking spaces or special characters\n",
    "        value_str = re.sub(r'[\\xa0\\u200b\\u200c\\u200d\\u2060]', '', value_str)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Check if it's a text value indicating not measured (e.g., \"gravidaként nem mértünk\")\n",
    "        value_lower = value_str.lower()\n",
    "        if any(keyword in value_lower for keyword in ['nem', 'not', 'mért', 'measured', 'n/a', 'na', 'gravidaként']):\n",
    "            return np.nan\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Replace comma decimal separator with dot (Hungarian format: 82,5 -> 82.5)\n",
    "        value_str = value_str.replace(',', '.')\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            return float(value_str)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_waist = df_seg['physical4_waist_circumference'].apply(extract_physical_waist)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_waist.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric waist circumference values from physical4_waist_circumference\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['physical4_waist_circumference'] = parsed_waist.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated physical4_waist_circumference column with parsed numeric values (dtype: {df_seg['physical4_waist_circumference'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in cv_screening8_waist_circumference if it exists and we have parsed values\n",
    "    if 'cv_screening8_waist_circumference' in df_seg.columns:\n",
    "        missing_cv_waist = df_seg['cv_screening8_waist_circumference'].isna()\n",
    "        can_fill = (missing_cv_waist & parsed_waist.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_cv_waist & parsed_waist.notna(), 'cv_screening8_waist_circumference'] = \\\n",
    "                parsed_waist[missing_cv_waist & parsed_waist.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing cv_screening8_waist_circumference values from physical4_waist_circumference\")\n",
    "    \n",
    "    print(f\"  physical4_waist_circumference now has {df_seg['physical4_waist_circumference'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['physical4_waist_circumference'].dtype}\")\n",
    "else:\n",
    "    print(\"  physical4_waist_circumference column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91b083cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing pulse_oximetry1_saturation to extract numeric values...\n",
      "  Extracted 2,687 numeric saturation values from pulse_oximetry1_saturation\n",
      "  Updated pulse_oximetry1_saturation column with parsed numeric values (dtype: float64)\n",
      "  pulse_oximetry1_saturation now has 2,687 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse pulse_oximetry1_saturation to extract numeric saturation values\n",
    "print(f\"\\nParsing pulse_oximetry1_saturation to extract numeric values...\")\n",
    "\n",
    "if 'pulse_oximetry1_saturation' in df_seg.columns:\n",
    "    def extract_saturation(value):\n",
    "        \"\"\"\n",
    "        Extract numeric oxygen saturation from pulse_oximetry1_saturation string.\n",
    "        Handles formats like:\n",
    "        - '99' -> 99 (simple number)\n",
    "        - '95%' -> 95 (remove percent sign)\n",
    "        - '97%' -> 97 (remove percent sign)\n",
    "        - ' 97' -> 97 (whitespace, clean and extract)\n",
    "        - '9886' -> NaN (data entry error, too high)\n",
    "        - '101', '104' -> NaN (above 100%, likely errors)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove percent signs\n",
    "        value_str = value_str.replace('%', '')\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            saturation = float(value_str)\n",
    "            # Filter out obvious errors:\n",
    "            # - Values > 100 (saturation can't exceed 100%)\n",
    "            # - Values > 1000 (likely data entry errors like '9886')\n",
    "            if saturation > 100:\n",
    "                return np.nan\n",
    "            return saturation\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_saturation = df_seg['pulse_oximetry1_saturation'].apply(extract_saturation)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_saturation.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric saturation values from pulse_oximetry1_saturation\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['pulse_oximetry1_saturation'] = parsed_saturation.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated pulse_oximetry1_saturation column with parsed numeric values (dtype: {df_seg['pulse_oximetry1_saturation'].dtype})\")\n",
    "    print(f\"  pulse_oximetry1_saturation now has {df_seg['pulse_oximetry1_saturation'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['pulse_oximetry1_saturation'].dtype}\")\n",
    "else:\n",
    "    print(\"  pulse_oximetry1_saturation column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14cf0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing pulse_oximetry2_pulse to extract numeric values...\n",
      "  Extracted 1,553 numeric pulse values from pulse_oximetry2_pulse\n",
      "  Updated pulse_oximetry2_pulse column with parsed numeric values (dtype: float64)\n",
      "  Filled 102 missing pulse values from pulse_oximetry2_pulse\n",
      "  pulse_oximetry2_pulse now has 1,553 non-null values\n",
      "  Final dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parse pulse_oximetry2_pulse to extract numeric pulse values\n",
    "print(f\"\\nParsing pulse_oximetry2_pulse to extract numeric values...\")\n",
    "\n",
    "if 'pulse_oximetry2_pulse' in df_seg.columns:\n",
    "    def extract_pulse(value):\n",
    "        \"\"\"\n",
    "        Extract numeric pulse rate from pulse_oximetry2_pulse string.\n",
    "        Handles formats like:\n",
    "        - '66' -> 66 (simple number)\n",
    "        - 79.0 -> 79.0 (already numeric)\n",
    "        - '66/min' -> 66 (remove unit)\n",
    "        - '9064.0' -> NaN (data entry error, too high for pulse rate)\n",
    "        - NaN -> NaN\n",
    "        \"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        # If already numeric, return as is\n",
    "        if isinstance(value, (int, float)):\n",
    "            pulse = float(value)\n",
    "            # Filter out obvious errors (pulse > 220 bpm based on outlier clipping range)\n",
    "            if pulse > 220:\n",
    "                return np.nan\n",
    "            return pulse\n",
    "        \n",
    "        # Convert to string if not already and strip whitespace\n",
    "        value_str = str(value).strip()\n",
    "        \n",
    "        # Remove units (/min, /minute, bpm, etc.)\n",
    "        value_str = re.sub(r'\\s*(/min|/minute|bpm|/h|/hr)\\s*$', '', value_str, flags=re.IGNORECASE)\n",
    "        value_str = value_str.strip()\n",
    "        \n",
    "        # Handle multiple numbers separated by space (take first)\n",
    "        if ' ' in value_str:\n",
    "            parts = value_str.split()\n",
    "            if len(parts) > 0:\n",
    "                value_str = parts[0]\n",
    "        \n",
    "        # Try to parse as float\n",
    "        try:\n",
    "            pulse = float(value_str)\n",
    "            # Filter out obvious errors (pulse > 220 bpm based on outlier clipping range)\n",
    "            if pulse > 220:\n",
    "                return np.nan\n",
    "            return pulse\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply extraction function to all values\n",
    "    parsed_pulse = df_seg['pulse_oximetry2_pulse'].apply(extract_pulse)\n",
    "    \n",
    "    # Count how many values we can extract\n",
    "    non_null_parsed = parsed_pulse.notna().sum()\n",
    "    print(f\"  Extracted {non_null_parsed:,} numeric pulse values from pulse_oximetry2_pulse\")\n",
    "    \n",
    "    # Replace the entire column with parsed numeric values\n",
    "    # This ensures ALL string values are converted to numbers (NaN for unparseable)\n",
    "    df_seg['pulse_oximetry2_pulse'] = parsed_pulse.astype('float64')\n",
    "    \n",
    "    print(f\"  Updated pulse_oximetry2_pulse column with parsed numeric values (dtype: {df_seg['pulse_oximetry2_pulse'].dtype})\")\n",
    "    \n",
    "    # Fill missing values in pulse column if it exists and we have parsed values\n",
    "    if 'pulse' in df_seg.columns:\n",
    "        missing_pulse = df_seg['pulse'].isna()\n",
    "        can_fill = (missing_pulse & parsed_pulse.notna()).sum()\n",
    "        \n",
    "        if can_fill > 0:\n",
    "            df_seg.loc[missing_pulse & parsed_pulse.notna(), 'pulse'] = \\\n",
    "                parsed_pulse[missing_pulse & parsed_pulse.notna()]\n",
    "            print(f\"  Filled {can_fill:,} missing pulse values from pulse_oximetry2_pulse\")\n",
    "    \n",
    "    print(f\"  pulse_oximetry2_pulse now has {df_seg['pulse_oximetry2_pulse'].notna().sum():,} non-null values\")\n",
    "    print(f\"  Final dtype: {df_seg['pulse_oximetry2_pulse'].dtype}\")\n",
    "else:\n",
    "    print(\"  pulse_oximetry2_pulse column not found, skipping parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "305b5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26508 entries, 0 to 26550\n",
      "Data columns (total 26 columns):\n",
      " #   Column                             Non-Null Count  Dtype                          \n",
      "---  ------                             --------------  -----                          \n",
      " 0   created                            26508 non-null  datetime64[ns, Europe/Budapest]\n",
      " 1   mep_region                         26508 non-null  object                         \n",
      " 2   patient_gender                     26508 non-null  object                         \n",
      " 3   prescribed_medication_atc          6681 non-null   object                         \n",
      " 4   specialty_name                     26508 non-null  object                         \n",
      " 5   measurements_ultrasound_category   4076 non-null   object                         \n",
      " 6   ultrasound3_date                   3794 non-null   datetime64[ns, Europe/Budapest]\n",
      " 7   bp_systolic_temp                   8294 non-null   float64                        \n",
      " 8   bp_systolic                        8294 non-null   float64                        \n",
      " 9   bp_diastolic                       8229 non-null   float64                        \n",
      " 10  bp_systolic2                       4990 non-null   float64                        \n",
      " 11  bp_diastolic2                      4885 non-null   float64                        \n",
      " 12  pulse                              8601 non-null   float64                        \n",
      " 13  cv_screening5_height               6343 non-null   float64                        \n",
      " 14  cv_screening6_weight               6539 non-null   float64                        \n",
      " 15  cv_screening7_bmi                  3242 non-null   float64                        \n",
      " 16  cv_screening8_waist_circumference  5538 non-null   float64                        \n",
      " 17  physical1_height                   6343 non-null   float64                        \n",
      " 18  physical2_weight                   6539 non-null   float64                        \n",
      " 19  physical3_bmi                      6311 non-null   float64                        \n",
      " 20  physical4_waist_circumference      5538 non-null   float64                        \n",
      " 21  pulse_oximetry1_saturation         2687 non-null   float64                        \n",
      " 22  pulse_oximetry2_pulse              1553 non-null   float64                        \n",
      " 23  icd3_code                          26494 non-null  object                         \n",
      " 24  pid                                26508 non-null  object                         \n",
      " 25  age                                26506 non-null  float64                        \n",
      "dtypes: datetime64[ns, Europe/Budapest](2), float64(17), object(7)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_seg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c6b46a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping rows with empty icd3_code...\n",
      "Shape before dropping: (26508, 26)\n",
      "Rows with missing/empty icd3_code: 14 (NaN: 14, empty string: 0)\n",
      "Rows removed: 14\n",
      "Shape after dropping: (26494, 26)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where icd3_code is empty\n",
    "print(f\"\\nDropping rows with empty icd3_code...\")\n",
    "print(f\"Shape before dropping: {df_seg.shape}\")\n",
    "\n",
    "if 'icd3_code' in df_seg.columns:\n",
    "    # Count rows with empty/missing icd3_code\n",
    "    empty_icd3 = df_seg['icd3_code'].isna().sum()\n",
    "    empty_icd3_str = (df_seg['icd3_code'] == '').sum()\n",
    "    total_empty = empty_icd3 + empty_icd3_str\n",
    "    \n",
    "    print(f\"Rows with missing/empty icd3_code: {total_empty:,} (NaN: {empty_icd3:,}, empty string: {empty_icd3_str:,})\")\n",
    "    \n",
    "    if total_empty > 0:\n",
    "        # Drop rows where icd3_code is NaN or empty string\n",
    "        df_seg = df_seg[(df_seg['icd3_code'].notna()) & (df_seg['icd3_code'] != '')].copy()\n",
    "        print(f\"Rows removed: {total_empty:,}\")\n",
    "    else:\n",
    "        print(\"No rows with empty icd3_code found ✓\")\n",
    "else:\n",
    "    print(\"icd3_code column not found, skipping\")\n",
    "\n",
    "print(f\"Shape after dropping: {df_seg.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6555991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier clipping with QA flags...\n",
      "Shape before clipping: (26494, 26)\n",
      "  bp_systolic2: 10 values clipped (10 below min, 0 above max)\n",
      "  bp_diastolic: 22 values clipped (14 below min, 8 above max)\n",
      "  bp_diastolic2: 12 values clipped (6 below min, 6 above max)\n",
      "  pulse: 2 values clipped (2 below min, 0 above max)\n",
      "  cv_screening5_height: 75 values clipped (75 below min, 0 above max)\n",
      "  physical1_height: 75 values clipped (75 below min, 0 above max)\n",
      "  cv_screening6_weight: 134 values clipped (134 below min, 0 above max)\n",
      "  physical2_weight: 134 values clipped (134 below min, 0 above max)\n",
      "  cv_screening7_bmi: 3 values clipped (0 below min, 3 above max)\n",
      "  physical3_bmi: 4 values clipped (1 below min, 3 above max)\n",
      "\n",
      "Clipping summary:\n",
      "  SBP: 10 values clipped (10 to min, 0 to max)\n",
      "  DBP: 34 values clipped (20 to min, 14 to max)\n",
      "  Pulse: 2 values clipped (2 to min, 0 to max)\n",
      "  Height: 150 values clipped (150 to min, 0 to max)\n",
      "  Weight: 268 values clipped (268 to min, 0 to max)\n",
      "  BMI: 7 values clipped (1 to min, 6 to max)\n",
      "\n",
      "Shape after clipping: (26494, 39)\n",
      "New QA flag columns created: 13\n"
     ]
    }
   ],
   "source": [
    "# Outlier clipping with QA flags\n",
    "print(f\"\\nOutlier clipping with QA flags...\")\n",
    "print(f\"Shape before clipping: {df_seg.shape}\")\n",
    "\n",
    "# Define clipping ranges for each measurement type\n",
    "clipping_config = {\n",
    "    'SBP': {'columns': ['bp_systolic', 'bp_systolic2', 'bp_systolic_temp'], 'min': 70, 'max': 260},\n",
    "    'DBP': {'columns': ['bp_diastolic', 'bp_diastolic2'], 'min': 40, 'max': 150},\n",
    "    'Pulse': {'columns': ['pulse', 'pulse_oximetry2_pulse'], 'min': 30, 'max': 220},\n",
    "    'Height': {'columns': ['cv_screening5_height', 'physical1_height'], 'min': 120, 'max': 220},\n",
    "    'Weight': {'columns': ['cv_screening6_weight', 'physical2_weight'], 'min': 30, 'max': 300},\n",
    "    'BMI': {'columns': ['cv_screening7_bmi', 'physical3_bmi'], 'min': 10, 'max': 80}\n",
    "}\n",
    "\n",
    "# Track clipping statistics\n",
    "clipping_stats = {}\n",
    "\n",
    "# Process each measurement type\n",
    "for measurement, config in clipping_config.items():\n",
    "    measurement_clipped = 0\n",
    "    measurement_clipped_min = 0\n",
    "    measurement_clipped_max = 0\n",
    "    \n",
    "    for col in config['columns']:\n",
    "        if col in df_seg.columns:\n",
    "            # Convert column to numeric first (handles string values, converts to NaN if not numeric)\n",
    "            df_seg[col] = pd.to_numeric(df_seg[col], errors='coerce')\n",
    "            \n",
    "            # Count values outside range (excluding NaN)\n",
    "            valid_mask = df_seg[col].notna()\n",
    "            if valid_mask.any():\n",
    "                below_min = (df_seg[col] < config['min']) & valid_mask\n",
    "                above_max = (df_seg[col] > config['max']) & valid_mask\n",
    "                \n",
    "                clipped_min_count = below_min.sum()\n",
    "                clipped_max_count = above_max.sum()\n",
    "                \n",
    "                # Clip values to boundaries (only for valid numeric values)\n",
    "                df_seg.loc[valid_mask, col] = np.clip(df_seg.loc[valid_mask, col], config['min'], config['max'])\n",
    "                \n",
    "                # Create QA flag column\n",
    "                flag_col = f'{col}_clipped'\n",
    "                df_seg[flag_col] = False\n",
    "                \n",
    "                # Mark clipped values in flag column\n",
    "                if clipped_min_count > 0:\n",
    "                    df_seg.loc[below_min, flag_col] = True\n",
    "                    measurement_clipped_min += clipped_min_count\n",
    "                \n",
    "                if clipped_max_count > 0:\n",
    "                    df_seg.loc[above_max, flag_col] = True\n",
    "                    measurement_clipped_max += clipped_max_count\n",
    "                \n",
    "                total_clipped = clipped_min_count + clipped_max_count\n",
    "                measurement_clipped += total_clipped\n",
    "                \n",
    "                if total_clipped > 0:\n",
    "                    print(f\"  {col}: {total_clipped:,} values clipped ({clipped_min_count:,} below min, {clipped_max_count:,} above max)\")\n",
    "    \n",
    "    if measurement_clipped > 0:\n",
    "        clipping_stats[measurement] = {\n",
    "            'total': measurement_clipped,\n",
    "            'min': measurement_clipped_min,\n",
    "            'max': measurement_clipped_max\n",
    "        }\n",
    "\n",
    "print(f\"\\nClipping summary:\")\n",
    "for measurement, stats in clipping_stats.items():\n",
    "    print(f\"  {measurement}: {stats['total']:,} values clipped ({stats['min']:,} to min, {stats['max']:,} to max)\")\n",
    "\n",
    "if not clipping_stats:\n",
    "    print(\"  No values required clipping ✓\")\n",
    "\n",
    "print(f\"\\nShape after clipping: {df_seg.shape}\")\n",
    "print(f\"New QA flag columns created: {sum(1 for col in df_seg.columns if col.endswith('_clipped'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5349388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Canonicalizing vital signs per encounter...\n",
      "Shape before canonicalization: (26494, 39)\n",
      "  SBP: 8,294 from bp_systolic, 818 from bp_systolic2, 0 from bp_systolic_temp\n",
      "  DBP: 8,229 from bp_diastolic, 820 from bp_diastolic2\n",
      "  Pulse: 8,601 from pulse, 0 from pulse_oximetry2_pulse\n",
      "  Height: 6,343 from cv_screening5_height, 0 from physical1_height\n",
      "  Weight: 6,539 from cv_screening6_weight, 0 from physical2_weight\n",
      "  BMI: 3,242 from cv_screening7_bmi, 3,098 from physical3_bmi, 1 recomputed\n",
      "\n",
      "Shape after canonicalization: (26494, 57)\n",
      "Canonical columns created:\n",
      "  - sbp_enc: 9,112 non-null values\n",
      "  - dbp_enc: 9,049 non-null values\n",
      "  - pulse_enc: 8,601 non-null values\n",
      "  - height_cm_enc: 6,343 non-null values\n",
      "  - weight_kg_enc: 6,539 non-null values\n",
      "  - bmi_enc: 6,341 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Canonicalize per encounter - create new canonical columns with source tracking\n",
    "print(f\"\\nCanonicalizing vital signs per encounter...\")\n",
    "print(f\"Shape before canonicalization: {df_seg.shape}\")\n",
    "\n",
    "# SBP priority: bp_systolic → bp_systolic2 → bp_systolic_temp\n",
    "df_seg['sbp_enc'] = np.nan\n",
    "df_seg['sbp_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['bp_systolic', 'bp_systolic2', 'bp_systolic_temp']):\n",
    "    # Priority 1: bp_systolic\n",
    "    mask1 = df_seg['bp_systolic'].notna()\n",
    "    df_seg.loc[mask1, 'sbp_enc'] = df_seg.loc[mask1, 'bp_systolic']\n",
    "    df_seg.loc[mask1, 'sbp_enc_source'] = 'bp_systolic'\n",
    "    \n",
    "    # Priority 2: bp_systolic2 (if still missing)\n",
    "    mask2 = df_seg['sbp_enc'].isna() & df_seg['bp_systolic2'].notna()\n",
    "    df_seg.loc[mask2, 'sbp_enc'] = df_seg.loc[mask2, 'bp_systolic2']\n",
    "    df_seg.loc[mask2, 'sbp_enc_source'] = 'bp_systolic2'\n",
    "    \n",
    "    # Priority 3: bp_systolic_temp (if still missing)\n",
    "    mask3 = df_seg['sbp_enc'].isna() & df_seg['bp_systolic_temp'].notna()\n",
    "    df_seg.loc[mask3, 'sbp_enc'] = df_seg.loc[mask3, 'bp_systolic_temp']\n",
    "    df_seg.loc[mask3, 'sbp_enc_source'] = 'bp_systolic_temp'\n",
    "    \n",
    "    print(f\"  SBP: {mask1.sum():,} from bp_systolic, {mask2.sum():,} from bp_systolic2, {mask3.sum():,} from bp_systolic_temp\")\n",
    "\n",
    "# DBP priority: bp_diastolic → bp_diastolic2\n",
    "df_seg['dbp_enc'] = np.nan\n",
    "df_seg['dbp_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['bp_diastolic', 'bp_diastolic2']):\n",
    "    # Priority 1: bp_diastolic\n",
    "    mask1 = df_seg['bp_diastolic'].notna()\n",
    "    df_seg.loc[mask1, 'dbp_enc'] = df_seg.loc[mask1, 'bp_diastolic']\n",
    "    df_seg.loc[mask1, 'dbp_enc_source'] = 'bp_diastolic'\n",
    "    \n",
    "    # Priority 2: bp_diastolic2 (if still missing)\n",
    "    mask2 = df_seg['dbp_enc'].isna() & df_seg['bp_diastolic2'].notna()\n",
    "    df_seg.loc[mask2, 'dbp_enc'] = df_seg.loc[mask2, 'bp_diastolic2']\n",
    "    df_seg.loc[mask2, 'dbp_enc_source'] = 'bp_diastolic2'\n",
    "    \n",
    "    print(f\"  DBP: {mask1.sum():,} from bp_diastolic, {mask2.sum():,} from bp_diastolic2\")\n",
    "\n",
    "# Pulse priority: pulse → pulse_oximetry2_pulse\n",
    "df_seg['pulse_enc'] = np.nan\n",
    "df_seg['pulse_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['pulse', 'pulse_oximetry2_pulse']):\n",
    "    # Priority 1: pulse\n",
    "    mask1 = df_seg['pulse'].notna()\n",
    "    df_seg.loc[mask1, 'pulse_enc'] = df_seg.loc[mask1, 'pulse']\n",
    "    df_seg.loc[mask1, 'pulse_enc_source'] = 'pulse'\n",
    "    \n",
    "    # Priority 2: pulse_oximetry2_pulse (if still missing)\n",
    "    mask2 = df_seg['pulse_enc'].isna() & df_seg['pulse_oximetry2_pulse'].notna()\n",
    "    df_seg.loc[mask2, 'pulse_enc'] = df_seg.loc[mask2, 'pulse_oximetry2_pulse']\n",
    "    df_seg.loc[mask2, 'pulse_enc_source'] = 'pulse_oximetry2_pulse'\n",
    "    \n",
    "    print(f\"  Pulse: {mask1.sum():,} from pulse, {mask2.sum():,} from pulse_oximetry2_pulse\")\n",
    "\n",
    "# Height priority: cv_screening5_height → physical1_height (in cm)\n",
    "df_seg['height_cm_enc'] = np.nan\n",
    "df_seg['height_cm_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['cv_screening5_height', 'physical1_height']):\n",
    "    # Priority 1: cv_screening5_height\n",
    "    mask1 = df_seg['cv_screening5_height'].notna()\n",
    "    df_seg.loc[mask1, 'height_cm_enc'] = df_seg.loc[mask1, 'cv_screening5_height']\n",
    "    df_seg.loc[mask1, 'height_cm_enc_source'] = 'cv_screening5_height'\n",
    "    \n",
    "    # Priority 2: physical1_height (if still missing)\n",
    "    mask2 = df_seg['height_cm_enc'].isna() & df_seg['physical1_height'].notna()\n",
    "    df_seg.loc[mask2, 'height_cm_enc'] = df_seg.loc[mask2, 'physical1_height']\n",
    "    df_seg.loc[mask2, 'height_cm_enc_source'] = 'physical1_height'\n",
    "    \n",
    "    print(f\"  Height: {mask1.sum():,} from cv_screening5_height, {mask2.sum():,} from physical1_height\")\n",
    "\n",
    "# Weight priority: cv_screening6_weight → physical2_weight (in kg)\n",
    "df_seg['weight_kg_enc'] = np.nan\n",
    "df_seg['weight_kg_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['cv_screening6_weight', 'physical2_weight']):\n",
    "    # Priority 1: cv_screening6_weight\n",
    "    mask1 = df_seg['cv_screening6_weight'].notna()\n",
    "    df_seg.loc[mask1, 'weight_kg_enc'] = df_seg.loc[mask1, 'cv_screening6_weight']\n",
    "    df_seg.loc[mask1, 'weight_kg_enc_source'] = 'cv_screening6_weight'\n",
    "    \n",
    "    # Priority 2: physical2_weight (if still missing)\n",
    "    mask2 = df_seg['weight_kg_enc'].isna() & df_seg['physical2_weight'].notna()\n",
    "    df_seg.loc[mask2, 'weight_kg_enc'] = df_seg.loc[mask2, 'physical2_weight']\n",
    "    df_seg.loc[mask2, 'weight_kg_enc_source'] = 'physical2_weight'\n",
    "    \n",
    "    print(f\"  Weight: {mask1.sum():,} from cv_screening6_weight, {mask2.sum():,} from physical2_weight\")\n",
    "\n",
    "# BMI priority: cv_screening7_bmi → physical3_bmi → (recompute if both missing but height+weight exist)\n",
    "df_seg['bmi_enc'] = np.nan\n",
    "df_seg['bmi_enc_source'] = ''\n",
    "\n",
    "if all(col in df_seg.columns for col in ['cv_screening7_bmi', 'physical3_bmi']):\n",
    "    # Priority 1: cv_screening7_bmi\n",
    "    mask1 = df_seg['cv_screening7_bmi'].notna()\n",
    "    df_seg.loc[mask1, 'bmi_enc'] = df_seg.loc[mask1, 'cv_screening7_bmi']\n",
    "    df_seg.loc[mask1, 'bmi_enc_source'] = 'cv_screening7_bmi'\n",
    "    \n",
    "    # Priority 2: physical3_bmi (if still missing)\n",
    "    mask2 = df_seg['bmi_enc'].isna() & df_seg['physical3_bmi'].notna()\n",
    "    df_seg.loc[mask2, 'bmi_enc'] = df_seg.loc[mask2, 'physical3_bmi']\n",
    "    df_seg.loc[mask2, 'bmi_enc_source'] = 'physical3_bmi'\n",
    "    \n",
    "    # Priority 3: Recompute from height and weight (if both missing but height+weight exist)\n",
    "    mask3 = (df_seg['bmi_enc'].isna() & \n",
    "             df_seg['height_cm_enc'].notna() & \n",
    "             df_seg['weight_kg_enc'].notna())\n",
    "    \n",
    "    if mask3.sum() > 0:\n",
    "        # BMI = weight (kg) / (height (m))^2\n",
    "        # Height is in cm, so convert to meters: height_m = height_cm / 100\n",
    "        df_seg.loc[mask3, 'bmi_enc'] = (\n",
    "            df_seg.loc[mask3, 'weight_kg_enc'] / \n",
    "            (df_seg.loc[mask3, 'height_cm_enc'] / 100) ** 2\n",
    "        )\n",
    "        df_seg.loc[mask3, 'bmi_enc_source'] = 'computed_from_height_weight'\n",
    "    \n",
    "    print(f\"  BMI: {mask1.sum():,} from cv_screening7_bmi, {mask2.sum():,} from physical3_bmi, {mask3.sum():,} recomputed\")\n",
    "\n",
    "# Create basic quality flags (True if value exists, False if missing)\n",
    "df_seg['sbp_enc_quality'] = df_seg['sbp_enc'].notna()\n",
    "df_seg['dbp_enc_quality'] = df_seg['dbp_enc'].notna()\n",
    "df_seg['pulse_enc_quality'] = df_seg['pulse_enc'].notna()\n",
    "df_seg['height_cm_enc_quality'] = df_seg['height_cm_enc'].notna()\n",
    "df_seg['weight_kg_enc_quality'] = df_seg['weight_kg_enc'].notna()\n",
    "df_seg['bmi_enc_quality'] = df_seg['bmi_enc'].notna()\n",
    "\n",
    "print(f\"\\nShape after canonicalization: {df_seg.shape}\")\n",
    "print(f\"Canonical columns created:\")\n",
    "print(f\"  - sbp_enc: {df_seg['sbp_enc'].notna().sum():,} non-null values\")\n",
    "print(f\"  - dbp_enc: {df_seg['dbp_enc'].notna().sum():,} non-null values\")\n",
    "print(f\"  - pulse_enc: {df_seg['pulse_enc'].notna().sum():,} non-null values\")\n",
    "print(f\"  - height_cm_enc: {df_seg['height_cm_enc'].notna().sum():,} non-null values\")\n",
    "print(f\"  - weight_kg_enc: {df_seg['weight_kg_enc'].notna().sum():,} non-null values\")\n",
    "print(f\"  - bmi_enc: {df_seg['bmi_enc'].notna().sum():,} non-null values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efaf4b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created', 'mep_region', 'patient_gender', 'prescribed_medication_atc',\n",
       "       'specialty_name', 'measurements_ultrasound_category',\n",
       "       'ultrasound3_date', 'bp_systolic_temp', 'bp_systolic', 'bp_diastolic',\n",
       "       'bp_systolic2', 'bp_diastolic2', 'pulse', 'cv_screening5_height',\n",
       "       'cv_screening6_weight', 'cv_screening7_bmi',\n",
       "       'cv_screening8_waist_circumference', 'physical1_height',\n",
       "       'physical2_weight', 'physical3_bmi', 'physical4_waist_circumference',\n",
       "       'pulse_oximetry1_saturation', 'pulse_oximetry2_pulse', 'icd3_code',\n",
       "       'pid', 'age', 'bp_systolic_clipped', 'bp_systolic2_clipped',\n",
       "       'bp_systolic_temp_clipped', 'bp_diastolic_clipped',\n",
       "       'bp_diastolic2_clipped', 'pulse_clipped',\n",
       "       'pulse_oximetry2_pulse_clipped', 'cv_screening5_height_clipped',\n",
       "       'physical1_height_clipped', 'cv_screening6_weight_clipped',\n",
       "       'physical2_weight_clipped', 'cv_screening7_bmi_clipped',\n",
       "       'physical3_bmi_clipped', 'sbp_enc', 'sbp_enc_source', 'dbp_enc',\n",
       "       'dbp_enc_source', 'pulse_enc', 'pulse_enc_source', 'height_cm_enc',\n",
       "       'height_cm_enc_source', 'weight_kg_enc', 'weight_kg_enc_source',\n",
       "       'bmi_enc', 'bmi_enc_source', 'sbp_enc_quality', 'dbp_enc_quality',\n",
       "       'pulse_enc_quality', 'height_cm_enc_quality', 'weight_kg_enc_quality',\n",
       "       'bmi_enc_quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd2b918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Creating patient-level summary...\n",
      "============================================================\n",
      "\n",
      "Grouping by pid...\n",
      "Total encounters: 26,494\n",
      "Unique patients: 6,356\n",
      "\n",
      "Patient summary created:\n",
      "  Shape: (6356, 39)\n",
      "  Columns: 39\n",
      "\n",
      "First few rows:\n",
      "                                                 pid  \\\n",
      "0  0022a311df715e9740427755bbc194533776e03242c47b...   \n",
      "1  00343c575db967dc335a85dab4ede661c58786aa3180df...   \n",
      "2  003d1426b37bb9341ba25c89156fee5ff70b6e0d402b36...   \n",
      "3  0045f4e1273379f7d0161bc42ce8f9028bdc7e7fa46581...   \n",
      "4  0048df6b40355e471b46ee1232b2d01c10c764a2be52fd...   \n",
      "\n",
      "           first_visit_date           last_visit_date  \\\n",
      "0 2024-08-16 09:49:24+02:00 2024-08-16 09:49:24+02:00   \n",
      "1 2023-08-15 10:51:17+02:00 2025-10-03 09:10:15+02:00   \n",
      "2 2025-04-02 09:51:05+02:00 2025-06-12 05:07:40+02:00   \n",
      "3 2023-08-07 13:09:16+02:00 2025-06-12 10:45:51+02:00   \n",
      "4 2023-08-09 12:12:50+02:00 2025-10-13 08:38:59+02:00   \n",
      "\n",
      "                 index_date  span_days  encounter_count_total  \\\n",
      "0 2024-08-16 09:49:24+02:00          0                      1   \n",
      "1 2025-10-03 09:10:15+02:00        779                      3   \n",
      "2 2025-06-12 05:07:40+02:00         70                     10   \n",
      "3 2025-06-12 10:45:51+02:00        674                     14   \n",
      "4 2025-10-13 08:38:59+02:00        795                     10   \n",
      "\n",
      "   encounter_count_12m  visits_per_year  sbp_latest  dbp_latest  ...  \\\n",
      "0                    1             1.00       109.0        76.0  ...   \n",
      "1                    2             1.41       141.0        74.0  ...   \n",
      "2                   10            52.18       126.0        83.0  ...   \n",
      "3                    8             7.59       116.0        78.0  ...   \n",
      "4                    6             4.59       150.0       100.0  ...   \n",
      "\n",
      "   thyroid_ultrasound_count  thyroid_ultrasound_last_date  bp_stage  \\\n",
      "0                         0                           NaT    Normal   \n",
      "1                         0                           NaT   Stage-2   \n",
      "2                         0                           NaT   Stage-1   \n",
      "3                         0                           NaT    Normal   \n",
      "4                         0                           NaT   Stage-2   \n",
      "\n",
      "     bmi_class  age_bracket  sbp_missing  dbp_missing  bmi_missing  \\\n",
      "0       Normal        25-40            0            0            0   \n",
      "1   Overweight        40-59            0            0            0   \n",
      "2    Obesity I        25-40            0            0            0   \n",
      "3   Overweight        40-59            0            0            0   \n",
      "4  Obesity II+        40-59            0            0            0   \n",
      "\n",
      "   pulse_missing  age_missing  \n",
      "0              0            0  \n",
      "1              0            0  \n",
      "2              0            0  \n",
      "3              0            0  \n",
      "4              0            0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Patient-level summary aggregation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Creating patient-level summary...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Helper functions for aggregations\n",
    "def get_latest_value(group, col, index_date):\n",
    "    \"\"\"Get most recent non-null value at or before index_date\"\"\"\n",
    "    valid = group[group[col].notna() & (group['created'] <= index_date)]\n",
    "    if len(valid) > 0:\n",
    "        return valid.sort_values('created', ascending=False).iloc[0][col]\n",
    "    return np.nan\n",
    "\n",
    "def get_baseline_value(group, col):\n",
    "    \"\"\"Get earliest non-null value\"\"\"\n",
    "    valid = group[group[col].notna()]\n",
    "    if len(valid) > 0:\n",
    "        return valid.sort_values('created', ascending=True).iloc[0][col]\n",
    "    return np.nan\n",
    "\n",
    "def get_most_recent(group, col):\n",
    "    \"\"\"Get most recent non-null value\"\"\"\n",
    "    valid = group[group[col].notna()]\n",
    "    if len(valid) > 0:\n",
    "        return valid.sort_values('created', ascending=False).iloc[0][col]\n",
    "    return np.nan\n",
    "\n",
    "# Group by patient\n",
    "print(f\"\\nGrouping by pid...\")\n",
    "print(f\"Total encounters: {len(df_seg):,}\")\n",
    "print(f\"Unique patients: {df_seg['pid'].nunique():,}\")\n",
    "\n",
    "# Initialize summary DataFrame\n",
    "patient_summary = []\n",
    "\n",
    "# Process each patient group\n",
    "for pid, group in df_seg.groupby('pid'):\n",
    "    # Sort by created date\n",
    "    group = group.sort_values('created')\n",
    "    \n",
    "    # Timeline & Utilization Metrics\n",
    "    first_visit_date = group['created'].min()\n",
    "    last_visit_date = group['created'].max()\n",
    "    index_date = last_visit_date\n",
    "    span_days = (last_visit_date - first_visit_date).days if first_visit_date != last_visit_date else 0\n",
    "    encounter_count_total = len(group)\n",
    "    \n",
    "    # Count encounters in last 12 months\n",
    "    date_12m_ago = index_date - pd.Timedelta(days=365)\n",
    "    encounter_count_12m = len(group[group['created'] >= date_12m_ago])\n",
    "    \n",
    "    # Visits per year\n",
    "    if span_days > 0:\n",
    "        visits_per_year = encounter_count_total / (span_days / 365.25)\n",
    "    else:\n",
    "        visits_per_year = encounter_count_total if encounter_count_total > 0 else 0\n",
    "    \n",
    "    # Latest state values (at index_date)\n",
    "    sbp_latest = get_latest_value(group, 'sbp_enc', index_date)\n",
    "    dbp_latest = get_latest_value(group, 'dbp_enc', index_date)\n",
    "    pulse_latest = get_latest_value(group, 'pulse_enc', index_date)\n",
    "    bmi_latest = get_latest_value(group, 'bmi_enc', index_date)\n",
    "    height_cm_latest = get_latest_value(group, 'height_cm_enc', index_date)\n",
    "    weight_kg_latest = get_latest_value(group, 'weight_kg_enc', index_date)\n",
    "    \n",
    "    # Baseline values (earliest non-null)\n",
    "    sbp_baseline = get_baseline_value(group, 'sbp_enc')\n",
    "    dbp_baseline = get_baseline_value(group, 'dbp_enc')\n",
    "    pulse_baseline = get_baseline_value(group, 'pulse_enc')\n",
    "    bmi_baseline = get_baseline_value(group, 'bmi_enc')\n",
    "    \n",
    "    # Delta values (latest - baseline)\n",
    "    sbp_delta = sbp_latest - sbp_baseline if (pd.notna(sbp_latest) and pd.notna(sbp_baseline)) else np.nan\n",
    "    dbp_delta = dbp_latest - dbp_baseline if (pd.notna(dbp_latest) and pd.notna(dbp_baseline)) else np.nan\n",
    "    pulse_delta = pulse_latest - pulse_baseline if (pd.notna(pulse_latest) and pd.notna(pulse_baseline)) else np.nan\n",
    "    bmi_delta = bmi_latest - bmi_baseline if (pd.notna(bmi_latest) and pd.notna(bmi_baseline)) else np.nan\n",
    "    \n",
    "    # Demographics (most recent known)\n",
    "    sex = get_most_recent(group, 'patient_gender')\n",
    "    mep_region = get_most_recent(group, 'mep_region')\n",
    "    \n",
    "    # Age at index_date\n",
    "    age_at_index = get_most_recent(group[group['created'] == index_date], 'age')\n",
    "    if pd.isna(age_at_index):\n",
    "        # Try to get age from any encounter and adjust\n",
    "        age_any = get_most_recent(group, 'age')\n",
    "        if pd.notna(age_any):\n",
    "            # Approximate age at index (using average encounter date)\n",
    "            avg_date = group['created'].mean()\n",
    "            days_diff = (index_date - avg_date).days\n",
    "            age_at_index = age_any + (days_diff / 365.25)\n",
    "    \n",
    "    # ICD-3 codes\n",
    "    icd3_codes = group['icd3_code'].dropna()\n",
    "    if len(icd3_codes) > 0:\n",
    "        # Count distinct codes\n",
    "        icd3_count = icd3_codes.nunique()\n",
    "        \n",
    "        # Most frequent ICD-3 (tie-break by most recent)\n",
    "        icd3_counts = icd3_codes.value_counts()\n",
    "        max_count = icd3_counts.max()\n",
    "        most_frequent_codes = icd3_counts[icd3_counts == max_count].index.tolist()\n",
    "        \n",
    "        if len(most_frequent_codes) == 1:\n",
    "            primary_icd3 = most_frequent_codes[0]\n",
    "        else:\n",
    "            # Tie-break: most recent\n",
    "            tie_group = group[group['icd3_code'].isin(most_frequent_codes)]\n",
    "            primary_icd3 = get_most_recent(tie_group, 'icd3_code')\n",
    "        \n",
    "        # Top 5 ICD-3 codes for list\n",
    "        top_5_codes = icd3_counts.head(5).index.tolist()\n",
    "        icd3_list = ', '.join(top_5_codes)\n",
    "    else:\n",
    "        primary_icd3 = np.nan\n",
    "        icd3_count = 0\n",
    "        icd3_list = ''\n",
    "    \n",
    "    # Ultrasound indicators\n",
    "    # Check if measurements_ultrasound_category contains \"thyroid\" (case-insensitive)\n",
    "    thyroid_ultrasound_rows = group[\n",
    "        group['measurements_ultrasound_category'].notna() & \n",
    "        group['measurements_ultrasound_category'].str.contains('thyroid', case=False, na=False)\n",
    "    ]\n",
    "    \n",
    "    thyroid_ultrasound_done = 1 if len(thyroid_ultrasound_rows) > 0 else 0\n",
    "    thyroid_ultrasound_count = len(thyroid_ultrasound_rows)\n",
    "    \n",
    "    if len(thyroid_ultrasound_rows) > 0:\n",
    "        thyroid_ultrasound_last_date = thyroid_ultrasound_rows['ultrasound3_date'].max()\n",
    "    else:\n",
    "        thyroid_ultrasound_last_date = pd.NaT\n",
    "    \n",
    "    # Interpretability bins\n",
    "    # BP Stage\n",
    "    if pd.notna(sbp_latest) and pd.notna(dbp_latest):\n",
    "        if sbp_latest < 120 and dbp_latest < 80:\n",
    "            bp_stage = \"Normal\"\n",
    "        elif 120 <= sbp_latest < 130 and dbp_latest < 80:\n",
    "            bp_stage = \"Elevated\"\n",
    "        elif (130 <= sbp_latest < 140 or 80 <= dbp_latest < 90) and not (sbp_latest >= 140 or dbp_latest >= 90):\n",
    "            bp_stage = \"Stage-1\"\n",
    "        elif sbp_latest >= 140 or dbp_latest >= 90:\n",
    "            bp_stage = \"Stage-2\"\n",
    "        else:\n",
    "            bp_stage = \"Missing\"\n",
    "    else:\n",
    "        bp_stage = \"Missing\"\n",
    "    \n",
    "    # BMI Class\n",
    "    if pd.notna(bmi_latest):\n",
    "        if 18.5 <= bmi_latest < 25:\n",
    "            bmi_class = \"Normal\"\n",
    "        elif 25 <= bmi_latest < 30:\n",
    "            bmi_class = \"Overweight\"\n",
    "        elif 30 <= bmi_latest < 35:\n",
    "            bmi_class = \"Obesity I\"\n",
    "        elif bmi_latest >= 35:\n",
    "            bmi_class = \"Obesity II+\"\n",
    "        else:\n",
    "            bmi_class = \"Missing\"\n",
    "    else:\n",
    "        bmi_class = \"Missing\"\n",
    "    \n",
    "    # Age Bracket\n",
    "    if pd.notna(age_at_index):\n",
    "        if age_at_index < 18:\n",
    "            age_bracket = \"0-18\"\n",
    "        elif 18 <= age_at_index < 25:\n",
    "            age_bracket = \"18-25\"\n",
    "        elif 25 <= age_at_index < 40:\n",
    "            age_bracket = \"25-40\"\n",
    "        elif 40 <= age_at_index < 60:\n",
    "            age_bracket = \"40-59\"\n",
    "        elif 60 <= age_at_index < 80:\n",
    "            age_bracket = \"60-79\"\n",
    "        elif age_at_index >= 80:\n",
    "            age_bracket = \"80+\"\n",
    "        else:\n",
    "            age_bracket = \"Missing\"\n",
    "    else:\n",
    "        age_bracket = \"Missing\"\n",
    "    \n",
    "    # Data quality indicators (missing flags)\n",
    "    sbp_missing = 1 if pd.isna(sbp_latest) else 0\n",
    "    dbp_missing = 1 if pd.isna(dbp_latest) else 0\n",
    "    bmi_missing = 1 if pd.isna(bmi_latest) else 0\n",
    "    pulse_missing = 1 if pd.isna(pulse_latest) else 0\n",
    "    age_missing = 1 if pd.isna(age_at_index) else 0\n",
    "    \n",
    "    # Store patient summary\n",
    "    patient_summary.append({\n",
    "        'pid': pid,\n",
    "        # Timeline & Utilization\n",
    "        'first_visit_date': first_visit_date,\n",
    "        'last_visit_date': last_visit_date,\n",
    "        'index_date': index_date,\n",
    "        'span_days': span_days,\n",
    "        'encounter_count_total': encounter_count_total,\n",
    "        'encounter_count_12m': encounter_count_12m,\n",
    "        'visits_per_year': round(visits_per_year, 2),\n",
    "        # Latest state\n",
    "        'sbp_latest': sbp_latest,\n",
    "        'dbp_latest': dbp_latest,\n",
    "        'pulse_latest': pulse_latest,\n",
    "        'bmi_latest': bmi_latest,\n",
    "        'height_cm_latest': height_cm_latest,\n",
    "        'weight_kg_latest': weight_kg_latest,\n",
    "        # Baseline\n",
    "        'sbp_baseline': sbp_baseline,\n",
    "        'dbp_baseline': dbp_baseline,\n",
    "        'pulse_baseline': pulse_baseline,\n",
    "        'bmi_baseline': bmi_baseline,\n",
    "        # Delta\n",
    "        'sbp_delta': sbp_delta,\n",
    "        'dbp_delta': dbp_delta,\n",
    "        'pulse_delta': pulse_delta,\n",
    "        'bmi_delta': bmi_delta,\n",
    "        # Demographics\n",
    "        'sex': sex,\n",
    "        'age': age_at_index,\n",
    "        'mep_region': mep_region,\n",
    "        # ICD-3\n",
    "        'primary_icd3': primary_icd3,\n",
    "        'icd3_count': icd3_count,\n",
    "        'icd3_list': icd3_list,\n",
    "        # Ultrasound\n",
    "        'thyroid_ultrasound_done': thyroid_ultrasound_done,\n",
    "        'thyroid_ultrasound_count': thyroid_ultrasound_count,\n",
    "        'thyroid_ultrasound_last_date': thyroid_ultrasound_last_date,\n",
    "        # Interpretability bins\n",
    "        'bp_stage': bp_stage,\n",
    "        'bmi_class': bmi_class,\n",
    "        'age_bracket': age_bracket,\n",
    "        # Data quality\n",
    "        'sbp_missing': sbp_missing,\n",
    "        'dbp_missing': dbp_missing,\n",
    "        'bmi_missing': bmi_missing,\n",
    "        'pulse_missing': pulse_missing,\n",
    "        'age_missing': age_missing\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_patient_summary = pd.DataFrame(patient_summary)\n",
    "\n",
    "print(f\"\\nPatient summary created:\")\n",
    "print(f\"  Shape: {df_patient_summary.shape}\")\n",
    "print(f\"  Columns: {len(df_patient_summary.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_patient_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "566092c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Adding icd3_count to df_patient_summary...\n",
      "============================================================\n",
      "✓ icd3_count already exists in DataFrame\n",
      "  Statistics: min=1, max=25, mean=3.03\n",
      "  Patients with 0 ICD-3 codes: 0\n",
      "  Patients with 1-2 ICD-3 codes: 3,560\n",
      "  Patients with 3+ ICD-3 codes: 2,796\n"
     ]
    }
   ],
   "source": [
    "# Add icd3_count: number of distinct ICD-3 codes the patient ever had\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Adding icd3_count to df_patient_summary...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Ensure icd3_count exists (it should already be in the DataFrame)\n",
    "# If for some reason it doesn't exist, we'll calculate it\n",
    "if 'icd3_count' not in df_patient_summary.columns:\n",
    "    print(\"Warning: icd3_count not found, calculating from original data...\")\n",
    "    # Calculate icd3_count from df_seg if needed\n",
    "    icd3_counts = df_seg.groupby('pid')['icd3_code'].nunique()\n",
    "    df_patient_summary['icd3_count'] = df_patient_summary['pid'].map(icd3_counts).fillna(0).astype(int)\n",
    "    print(f\"✓ icd3_count calculated and added\")\n",
    "else:\n",
    "    print(f\"✓ icd3_count already exists in DataFrame\")\n",
    "\n",
    "print(f\"  Statistics: min={df_patient_summary['icd3_count'].min()}, max={df_patient_summary['icd3_count'].max()}, mean={df_patient_summary['icd3_count'].mean():.2f}\")\n",
    "print(f\"  Patients with 0 ICD-3 codes: {(df_patient_summary['icd3_count'] == 0).sum():,}\")\n",
    "print(f\"  Patients with 1-2 ICD-3 codes: {((df_patient_summary['icd3_count'] >= 1) & (df_patient_summary['icd3_count'] < 3)).sum():,}\")\n",
    "print(f\"  Patients with 3+ ICD-3 codes: {(df_patient_summary['icd3_count'] >= 3).sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b9ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Adding multimorbidity_flag to df_patient_summary...\n",
      "============================================================\n",
      "✓ multimorbidity_flag added\n",
      "  Patients with multimorbidity (icd3_count >= 3): 2,796 (44.0%)\n",
      "  Patients without multimorbidity (icd3_count < 3): 3,560 (56.0%)\n",
      "\n",
      "Updated DataFrame shape: (6356, 40)\n",
      "Updated columns: 40\n"
     ]
    }
   ],
   "source": [
    "# Add multimorbidity_flag: 1 if icd3_count >= 3, else 0\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Adding multimorbidity_flag to df_patient_summary...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Ensure icd3_count exists before creating the flag\n",
    "if 'icd3_count' not in df_patient_summary.columns:\n",
    "    raise ValueError(\"icd3_count must exist in df_patient_summary before creating multimorbidity_flag\")\n",
    "\n",
    "# Add multimorbidity_flag: 1 if icd3_count >= 3, else 0\n",
    "df_patient_summary['multimorbidity_flag'] = (df_patient_summary['icd3_count'] >= 3).astype(int)\n",
    "\n",
    "print(f\"✓ multimorbidity_flag added\")\n",
    "print(f\"  Patients with multimorbidity (icd3_count >= 3): {df_patient_summary['multimorbidity_flag'].sum():,} ({df_patient_summary['multimorbidity_flag'].mean()*100:.1f}%)\")\n",
    "print(f\"  Patients without multimorbidity (icd3_count < 3): {(df_patient_summary['multimorbidity_flag'] == 0).sum():,} ({(df_patient_summary['multimorbidity_flag'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"\\nUpdated DataFrame shape: {df_patient_summary.shape}\")\n",
    "print(f\"Updated columns: {len(df_patient_summary.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc055be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Saving patient summary to CSV...\n",
      "============================================================\n",
      "\n",
      "✓ Patient summary saved to: ../data/patient_summary.csv\n",
      "  Shape: (6356, 39)\n",
      "  Total patients: 6,356\n",
      "\n",
      "Summary statistics:\n",
      "  Patients with SBP: 4,003\n",
      "  Patients with DBP: 3,982\n",
      "  Patients with BMI: 3,924\n",
      "  Patients with thyroid ultrasound: 0\n",
      "\n",
      "Column names:\n",
      "   1. pid\n",
      "   2. first_visit_date\n",
      "   3. last_visit_date\n",
      "   4. index_date\n",
      "   5. span_days\n",
      "   6. encounter_count_total\n",
      "   7. encounter_count_12m\n",
      "   8. visits_per_year\n",
      "   9. sbp_latest\n",
      "  10. dbp_latest\n",
      "  11. pulse_latest\n",
      "  12. bmi_latest\n",
      "  13. height_cm_latest\n",
      "  14. weight_kg_latest\n",
      "  15. sbp_baseline\n",
      "  16. dbp_baseline\n",
      "  17. pulse_baseline\n",
      "  18. bmi_baseline\n",
      "  19. sbp_delta\n",
      "  20. dbp_delta\n",
      "  21. pulse_delta\n",
      "  22. bmi_delta\n",
      "  23. sex\n",
      "  24. age\n",
      "  25. mep_region\n",
      "  26. primary_icd3\n",
      "  27. icd3_count\n",
      "  28. icd3_list\n",
      "  29. thyroid_ultrasound_done\n",
      "  30. thyroid_ultrasound_count\n",
      "  31. thyroid_ultrasound_last_date\n",
      "  32. bp_stage\n",
      "  33. bmi_class\n",
      "  34. age_bracket\n",
      "  35. sbp_missing\n",
      "  36. dbp_missing\n",
      "  37. bmi_missing\n",
      "  38. pulse_missing\n",
      "  39. age_missing\n"
     ]
    }
   ],
   "source": [
    "# Save patient summary to CSV\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Saving patient summary to CSV...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Ensure proper column order\n",
    "column_order = [\n",
    "    'pid',\n",
    "    # Timeline & Utilization\n",
    "    'first_visit_date', 'last_visit_date', 'index_date', 'span_days',\n",
    "    'encounter_count_total', 'encounter_count_12m', 'visits_per_year',\n",
    "    # Latest state\n",
    "    'sbp_latest', 'dbp_latest', 'pulse_latest', \n",
    "    'bmi_latest', 'height_cm_latest', 'weight_kg_latest',\n",
    "    # Baseline\n",
    "    'sbp_baseline', 'dbp_baseline', 'pulse_baseline', 'bmi_baseline',\n",
    "    # Delta\n",
    "    'sbp_delta', 'dbp_delta', 'pulse_delta', 'bmi_delta',\n",
    "    # Demographics\n",
    "    'sex', 'age', 'mep_region',\n",
    "    # ICD-3\n",
    "    'primary_icd3', 'icd3_count', 'icd3_list',\n",
    "    # Ultrasound\n",
    "    'thyroid_ultrasound_done', 'thyroid_ultrasound_count', 'thyroid_ultrasound_last_date',\n",
    "    # Interpretability bins\n",
    "    'bp_stage', 'bmi_class', 'age_bracket',\n",
    "    # Data quality\n",
    "    'sbp_missing', 'dbp_missing', 'bmi_missing', 'pulse_missing', 'age_missing'\n",
    "]\n",
    "\n",
    "# Reorder columns (only include columns that exist)\n",
    "df_patient_summary_final = df_patient_summary[[col for col in column_order if col in df_patient_summary.columns]]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path('../data/patient_summary.csv')\n",
    "df_patient_summary_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Patient summary saved to: {output_path}\")\n",
    "print(f\"  Shape: {df_patient_summary_final.shape}\")\n",
    "print(f\"  Total patients: {len(df_patient_summary_final):,}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(f\"  Patients with SBP: {df_patient_summary_final['sbp_latest'].notna().sum():,}\")\n",
    "print(f\"  Patients with DBP: {df_patient_summary_final['dbp_latest'].notna().sum():,}\")\n",
    "print(f\"  Patients with BMI: {df_patient_summary_final['bmi_latest'].notna().sum():,}\")\n",
    "print(f\"  Patients with thyroid ultrasound: {df_patient_summary_final['thyroid_ultrasound_done'].sum():,}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_patient_summary_final.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fbc2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Checking for duplicates in patient summary...\n",
      "============================================================\n",
      "\n",
      "Total rows: 6,356\n",
      "Unique pids: 6,356\n",
      "Duplicate pids: 0\n",
      "\n",
      "✓ No duplicate pids found - each patient has exactly one row\n",
      "\n",
      "Completely duplicate rows (all columns identical): 0\n",
      "✓ No completely duplicate rows found\n",
      "\n",
      "============================================================\n",
      "Checking for outliers in numerical fields...\n",
      "============================================================\n",
      "\n",
      "⚠️ Found outliers in 19 numerical fields:\n",
      "\n",
      "  span_days:\n",
      "    Total outliers: 616 (9.69%)\n",
      "    Low outliers: 0 (values < -334.88)\n",
      "    High outliers: 616 (values > 558.12)\n",
      "    Range: [0.00, 975.00]\n",
      "    Q1: 0.00, Median: 62.00, Q3: 223.25\n",
      "\n",
      "  encounter_count_total:\n",
      "    Total outliers: 415 (6.53%)\n",
      "    Low outliers: 0 (values < -5.00)\n",
      "    High outliers: 415 (values > 11.00)\n",
      "    Range: [1.00, 93.00]\n",
      "    Q1: 1.00, Median: 3.00, Q3: 5.00\n",
      "\n",
      "  encounter_count_12m:\n",
      "    Total outliers: 421 (6.62%)\n",
      "    Low outliers: 0 (values < -3.50)\n",
      "    High outliers: 421 (values > 8.50)\n",
      "    Range: [1.00, 69.00]\n",
      "    Q1: 1.00, Median: 2.00, Q3: 4.00\n",
      "\n",
      "  visits_per_year:\n",
      "    Total outliers: 734 (11.55%)\n",
      "    Low outliers: 0 (values < -26.33)\n",
      "    High outliers: 734 (values > 46.55)\n",
      "    Range: [0.88, 1095.75]\n",
      "    Q1: 1.00, Median: 7.61, Q3: 19.22\n",
      "\n",
      "  sbp_latest:\n",
      "    Total outliers: 71 (1.12%)\n",
      "    Low outliers: 4 (values < 80.50)\n",
      "    High outliers: 67 (values > 180.50)\n",
      "    Range: [77.00, 229.00]\n",
      "    Q1: 118.00, Median: 130.00, Q3: 143.00\n",
      "\n",
      "  dbp_latest:\n",
      "    Total outliers: 75 (1.18%)\n",
      "    Low outliers: 26 (values < 53.50)\n",
      "    High outliers: 49 (values > 113.50)\n",
      "    Range: [40.00, 150.00]\n",
      "    Q1: 76.00, Median: 83.00, Q3: 91.00\n",
      "\n",
      "  pulse_latest:\n",
      "    Total outliers: 48 (0.76%)\n",
      "    Low outliers: 1 (values < 45.50)\n",
      "    High outliers: 47 (values > 113.50)\n",
      "    Range: [45.00, 165.00]\n",
      "    Q1: 71.00, Median: 80.00, Q3: 88.00\n",
      "\n",
      "  bmi_latest:\n",
      "    Total outliers: 62 (0.98%)\n",
      "    Low outliers: 0 (values < 9.84)\n",
      "    High outliers: 62 (values > 46.72)\n",
      "    Range: [10.02, 80.00]\n",
      "    Q1: 23.67, Median: 28.12, Q3: 32.89\n",
      "\n",
      "  height_cm_latest:\n",
      "    Total outliers: 118 (1.86%)\n",
      "    Low outliers: 97 (values < 135.00)\n",
      "    High outliers: 21 (values > 191.00)\n",
      "    Range: [120.00, 198.00]\n",
      "    Q1: 156.00, Median: 163.00, Q3: 170.00\n",
      "\n",
      "  weight_kg_latest:\n",
      "    Total outliers: 50 (0.79%)\n",
      "    Low outliers: 0 (values < 20.00)\n",
      "    High outliers: 50 (values > 132.00)\n",
      "    Range: [30.00, 180.00]\n",
      "    Q1: 62.00, Median: 75.00, Q3: 90.00\n",
      "\n",
      "  sbp_baseline:\n",
      "    Total outliers: 58 (0.91%)\n",
      "    Low outliers: 0 (values < 77.00)\n",
      "    High outliers: 58 (values > 189.00)\n",
      "    Range: [80.00, 230.00]\n",
      "    Q1: 119.00, Median: 132.00, Q3: 147.00\n",
      "\n",
      "  dbp_baseline:\n",
      "    Total outliers: 71 (1.12%)\n",
      "    Low outliers: 14 (values < 51.00)\n",
      "    High outliers: 57 (values > 115.00)\n",
      "    Range: [40.00, 150.00]\n",
      "    Q1: 75.00, Median: 83.00, Q3: 91.00\n",
      "\n",
      "  pulse_baseline:\n",
      "    Total outliers: 56 (0.88%)\n",
      "    Low outliers: 1 (values < 44.88)\n",
      "    High outliers: 55 (values > 111.88)\n",
      "    Range: [42.00, 165.00]\n",
      "    Q1: 70.00, Median: 78.00, Q3: 86.75\n",
      "\n",
      "  bmi_baseline:\n",
      "    Total outliers: 66 (1.04%)\n",
      "    Low outliers: 0 (values < 9.84)\n",
      "    High outliers: 66 (values > 46.72)\n",
      "    Range: [10.00, 80.00]\n",
      "    Q1: 23.67, Median: 28.16, Q3: 32.89\n",
      "\n",
      "  sbp_delta:\n",
      "    Total outliers: 1,157 (18.20%)\n",
      "    Low outliers: 616 (values < -12.50)\n",
      "    High outliers: 541 (values > 7.50)\n",
      "    Range: [-88.00, 71.00]\n",
      "    Q1: -5.00, Median: 0.00, Q3: 0.00\n",
      "\n",
      "  dbp_delta:\n",
      "    Total outliers: 1,811 (28.49%)\n",
      "    Low outliers: 896 (values < -2.50)\n",
      "    High outliers: 915 (values > 1.50)\n",
      "    Range: [-65.00, 86.00]\n",
      "    Q1: -1.00, Median: 0.00, Q3: 0.00\n",
      "\n",
      "  pulse_delta:\n",
      "    Total outliers: 931 (14.65%)\n",
      "    Low outliers: 411 (values < -6.00)\n",
      "    High outliers: 520 (values > 10.00)\n",
      "    Range: [-57.00, 68.00]\n",
      "    Q1: 0.00, Median: 0.00, Q3: 4.00\n",
      "\n",
      "  bmi_delta:\n",
      "    Total outliers: 282 (4.44%)\n",
      "    Low outliers: 162 (values < 0.00)\n",
      "    High outliers: 120 (values > 0.00)\n",
      "    Range: [-50.62, 28.39]\n",
      "    Q1: 0.00, Median: 0.00, Q3: 0.00\n",
      "\n",
      "  icd3_count:\n",
      "    Total outliers: 285 (4.48%)\n",
      "    Low outliers: 0 (values < -3.50)\n",
      "    High outliers: 285 (values > 8.50)\n",
      "    Range: [1.00, 25.00]\n",
      "    Q1: 1.00, Median: 2.00, Q3: 4.00\n",
      "\n",
      "\n",
      "Sample of extreme outliers (top 5 per field):\n",
      "\n",
      "  span_days:\n",
      "    Top 5 high outliers:\n",
      "      pid=80d488452138b4c721caf321062f29c7347df70473555e6b185700792001f1c9: span_days=975.00\n",
      "      pid=15b7abdc93607f7830cd70a7aeef7754eaac67b84ead62121356a54a7968afd9: span_days=972.00\n",
      "      pid=15b6239fce15d9ccc04a4393166b05a541b212fbb2c8b83701709d569f33eef8: span_days=970.00\n",
      "      pid=12ce11bc12b552c517b3d849e36b3506cc6048bfe66343687c9a7183e77358d2: span_days=960.00\n",
      "      pid=c9ac6bce54647d4cf9d8d97a8fafde0234a92c739b23a01eecaec9310c4a77f7: span_days=953.00\n",
      "\n",
      "  encounter_count_total:\n",
      "    Top 5 high outliers:\n",
      "      pid=de3b01f5759a3eac648caadeef71c423de3826a48fe705a1079fedb2f7788351: encounter_count_total=93.00\n",
      "      pid=8d17759f93848ab1b8b1a2dca93308a46f99c45a7dd0934a027d02e0e87210e6: encounter_count_total=46.00\n",
      "      pid=d2dfbac4db2be239aed1bd2f23d14261149f89142560cc0c61c08f409ee1eb5c: encounter_count_total=41.00\n",
      "      pid=a9fe57593b299c0213f260f55eedd21d219f0d568569a13d8c9936d3540a3692: encounter_count_total=40.00\n",
      "      pid=8dba12966c8c5539a752ab23cca932be0ca3127043452cac13c2eda6747bcbb8: encounter_count_total=39.00\n",
      "\n",
      "  encounter_count_12m:\n",
      "    Top 5 high outliers:\n",
      "      pid=de3b01f5759a3eac648caadeef71c423de3826a48fe705a1079fedb2f7788351: encounter_count_12m=69.00\n",
      "      pid=ca3112e48bacf6b60f87a616d5d1ce9b932b899458faee57805e6950e06afa2e: encounter_count_12m=30.00\n",
      "      pid=632e68650da3915c91c7c36bfd6f70c50ed83c7c5295047fd7ad90ee7f89bbf8: encounter_count_12m=26.00\n",
      "      pid=45861477877df87484ea8993a0a3fd2b0d4f8448b8ff5ae18464fbc25fb30071: encounter_count_12m=25.00\n",
      "      pid=1b17c23c59fd61cbfec281e965e6bec1b09e7f6ec5491bb17056e7b37ac8bb40: encounter_count_12m=22.00\n",
      "\n",
      "  visits_per_year:\n",
      "    Top 5 high outliers:\n",
      "      pid=1b5635ffad09fb1b30250575d42203563ed41f69f88ea138891e1823001f363a: visits_per_year=1095.75\n",
      "      pid=0d63ae127a854b8489e2c424d8f2b25f363be6ceb6f781cc099e89da32a8d939: visits_per_year=730.50\n",
      "      pid=2c68dcf6d9ddbb9986b364ac20f56f9a17d931c7f86c2e92c602877e4e2c5a52: visits_per_year=730.50\n",
      "      pid=341e3d59fc3c2fdfd3617bcb5d441b92c60a9d3e574abf240cf548f81a382720: visits_per_year=730.50\n",
      "      pid=4424d83103a5b00010dfafa16075140a17450e767e325997ae7fd1f57d85da59: visits_per_year=730.50\n",
      "\n",
      "  sbp_latest:\n",
      "    Top 5 high outliers:\n",
      "      pid=d4f661ba0bc2fde8556f40fdc611a0cf8c9d9980335e3db4fb16e23ba6702bc9: sbp_latest=229.00\n",
      "      pid=c6ba680139eee8465a7587f4064c7fa50d86a952dfd68f58fb0a3b36e9d1a076: sbp_latest=220.00\n",
      "      pid=ba6bd78c5a3bfbe9ae0218dc1371dff9192cb65f4685fdd0a2d7a9f06a6b6203: sbp_latest=215.00\n",
      "      pid=5f9ff325d64ea3b0b35e633d6125820a9c52bc76dc8ce8a6ca83d82e9bbd38ac: sbp_latest=212.00\n",
      "      pid=bc43e2a4bd284acffa7a5428f7a109dd492249846d06699ba631c9ff20877bb1: sbp_latest=211.00\n",
      "    Top 5 low outliers:\n",
      "      pid=b1a0d6a6014831c9fbd19fd9f4c3fce4ab08df8ea885833742ef6cf0ddbc8b1d: sbp_latest=77.00\n",
      "      pid=5b5a0a1b8054cb4ed545b4ed24278e6dfd7558b28e1ff574a5ed6f1a1608d51b: sbp_latest=80.00\n",
      "      pid=671d34f1db65b3d2afccbd1f0b58b193ee05596abcc5dbe26e096844f0032241: sbp_latest=80.00\n",
      "      pid=6af2341528febff614a9aa0f6ad9777969e6597b2324fa22023aa97823d39940: sbp_latest=80.00\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and outliers in patient summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Checking for duplicates in patient summary...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check for duplicate pids (should be unique - one row per patient)\n",
    "duplicate_pids = df_patient_summary_final['pid'].duplicated().sum()\n",
    "total_patients = len(df_patient_summary_final)\n",
    "unique_pids = df_patient_summary_final['pid'].nunique()\n",
    "\n",
    "print(f\"\\nTotal rows: {total_patients:,}\")\n",
    "print(f\"Unique pids: {unique_pids:,}\")\n",
    "print(f\"Duplicate pids: {duplicate_pids:,}\")\n",
    "\n",
    "if duplicate_pids > 0:\n",
    "    print(f\"\\n⚠️ WARNING: Found {duplicate_pids} duplicate patient IDs!\")\n",
    "    print(\"\\nDuplicate pids:\")\n",
    "    duplicate_pid_list = df_patient_summary_final[df_patient_summary_final['pid'].duplicated(keep=False)]['pid'].unique()\n",
    "    print(duplicate_pid_list)\n",
    "    \n",
    "    # Show the duplicate rows\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(df_patient_summary_final[df_patient_summary_final['pid'].duplicated(keep=False)].sort_values('pid'))\n",
    "else:\n",
    "    print(\"\\n✓ No duplicate pids found - each patient has exactly one row\")\n",
    "\n",
    "# Also check for completely duplicate rows (all columns the same)\n",
    "duplicate_rows = df_patient_summary_final.duplicated().sum()\n",
    "print(f\"\\nCompletely duplicate rows (all columns identical): {duplicate_rows:,}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"\\n⚠️ WARNING: Found completely duplicate rows!\")\n",
    "    print(df_patient_summary_final[df_patient_summary_final.duplicated(keep=False)].sort_values('pid'))\n",
    "else:\n",
    "    print(\"✓ No completely duplicate rows found\")\n",
    "\n",
    "# Check for outliers in numerical fields\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Checking for outliers in numerical fields...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Define numerical columns to check for outliers\n",
    "numerical_cols = [\n",
    "    'span_days', 'encounter_count_total', 'encounter_count_12m', 'visits_per_year',\n",
    "    'sbp_latest', 'dbp_latest', 'pulse_latest', 'bmi_latest', \n",
    "    'height_cm_latest', 'weight_kg_latest',\n",
    "    'sbp_baseline', 'dbp_baseline', 'pulse_baseline', 'bmi_baseline',\n",
    "    'sbp_delta', 'dbp_delta', 'pulse_delta', 'bmi_delta',\n",
    "    'age', 'icd3_count', 'thyroid_ultrasound_count'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist in the DataFrame\n",
    "numerical_cols = [col for col in numerical_cols if col in df_patient_summary_final.columns]\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    # Skip if column is all NaN\n",
    "    if df_patient_summary_final[col].notna().sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate IQR-based outliers\n",
    "    Q1 = df_patient_summary_final[col].quantile(0.25)\n",
    "    Q3 = df_patient_summary_final[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds (1.5 * IQR rule)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers_low = df_patient_summary_final[df_patient_summary_final[col] < lower_bound]\n",
    "    outliers_high = df_patient_summary_final[df_patient_summary_final[col] > upper_bound]\n",
    "    total_outliers = len(outliers_low) + len(outliers_high)\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        outlier_summary.append({\n",
    "            'column': col,\n",
    "            'total_outliers': total_outliers,\n",
    "            'outliers_low': len(outliers_low),\n",
    "            'outliers_high': len(outliers_high),\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'min_value': df_patient_summary_final[col].min(),\n",
    "            'max_value': df_patient_summary_final[col].max(),\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'median': df_patient_summary_final[col].median()\n",
    "        })\n",
    "\n",
    "if len(outlier_summary) > 0:\n",
    "    print(f\"\\n⚠️ Found outliers in {len(outlier_summary)} numerical fields:\\n\")\n",
    "    \n",
    "    for item in outlier_summary:\n",
    "        print(f\"  {item['column']}:\")\n",
    "        print(f\"    Total outliers: {item['total_outliers']:,} ({item['total_outliers']/total_patients*100:.2f}%)\")\n",
    "        print(f\"    Low outliers: {item['outliers_low']:,} (values < {item['lower_bound']:.2f})\")\n",
    "        print(f\"    High outliers: {item['outliers_high']:,} (values > {item['upper_bound']:.2f})\")\n",
    "        print(f\"    Range: [{item['min_value']:.2f}, {item['max_value']:.2f}]\")\n",
    "        print(f\"    Q1: {item['Q1']:.2f}, Median: {item['median']:.2f}, Q3: {item['Q3']:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # Show sample of extreme outliers (top 5 highest and lowest for each field with outliers)\n",
    "    print(\"\\nSample of extreme outliers (top 5 per field):\")\n",
    "    for item in outlier_summary[:5]:  # Show first 5 fields to avoid too much output\n",
    "        col = item['column']\n",
    "        print(f\"\\n  {col}:\")\n",
    "        \n",
    "        # High outliers\n",
    "        high_outliers = df_patient_summary_final[df_patient_summary_final[col] > item['upper_bound']].nlargest(5, col)\n",
    "        if len(high_outliers) > 0:\n",
    "            print(f\"    Top 5 high outliers:\")\n",
    "            for idx, row in high_outliers.iterrows():\n",
    "                print(f\"      pid={row['pid']}: {col}={row[col]:.2f}\")\n",
    "        \n",
    "        # Low outliers\n",
    "        low_outliers = df_patient_summary_final[df_patient_summary_final[col] < item['lower_bound']].nsmallest(5, col)\n",
    "        if len(low_outliers) > 0:\n",
    "            print(f\"    Top 5 low outliers:\")\n",
    "            for idx, row in low_outliers.iterrows():\n",
    "                print(f\"      pid={row['pid']}: {col}={row[col]:.2f}\")\n",
    "else:\n",
    "    print(\"\\n✓ No outliers detected using IQR method (1.5 * IQR rule)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2ad80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
